{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ALADIN- \\alpha \\alpha ALADIN- \\alpha \\alpha is a rapid-prototyping toolbox for distributed and decentralized non-convex optimization. ALADIN- \\alpha \\alpha provides an implementation of the Augmented Lagrangian Alternating Direction Inexact Newton (ALADIN) algorithm and the Alternating Direction of Multipliers Method (ADMM) with a unified interface. Moreover, a bi-level ALADIN variant is included in ALADIN- \\alpha \\alpha allowing for decentralized non-convex optimization. Application examples from various fields highlight the broad applicability of ALADIN- \\alpha \\alpha . The toolbox is described in more detail here . ALADIN- \\alpha \\alpha solves problem of the form \\begin{aligned} &\\min_{x_1,\\dots,x_{n_s}} && \\sum_{i\\in \\mathcal{S}} f_i(x_i,p_i) \\\\ &\\;\\;\\text{subject to}&&g_{i}(x_i,p_i) = 0 \\quad &&\\mid \\kappa_i, &\\forall i \\in \\mathcal{R}, \\\\ &&&h_{i}(x_i,p_i) \\leq 0 \\quad \\;\\,&& \\mid \\gamma_i, &\\forall i \\in \\mathcal{R}, \\\\ &&&\\underline{x}_i \\leq x_i \\leq \\overline{x}_i\\;\\,&& \\mid\\eta_i, &\\forall i \\in \\mathcal{R}, \\\\ &&&\\sum_{i\\in \\mathcal{S}}A_i x_i=0\\;&&\\mid\\lambda. \\end{aligned} \\begin{aligned} &\\min_{x_1,\\dots,x_{n_s}} && \\sum_{i\\in \\mathcal{S}} f_i(x_i,p_i) \\\\ &\\;\\;\\text{subject to}&&g_{i}(x_i,p_i) = 0 \\quad &&\\mid \\kappa_i, &\\forall i \\in \\mathcal{R}, \\\\ &&&h_{i}(x_i,p_i) \\leq 0 \\quad \\;\\,&& \\mid \\gamma_i, &\\forall i \\in \\mathcal{R}, \\\\ &&&\\underline{x}_i \\leq x_i \\leq \\overline{x}_i\\;\\,&& \\mid\\eta_i, &\\forall i \\in \\mathcal{R}, \\\\ &&&\\sum_{i\\in \\mathcal{S}}A_i x_i=0\\;&&\\mid\\lambda. \\end{aligned} in a distributed fashion. Eearly-stage version of ALADIN- \\alpha \\alpha Note that ALADIN- \\alpha \\alpha is still in a prototypical phase of development. An example Here\u2019s an example how to use ALADIN- \\alpha \\alpha . Let us consider an inequality-constrained non-convex problem \\begin{aligned} & \\min_{x \\in \\mathbb{R},y \\in \\mathbb{R}^2} 2 \\,(x - 1)^2 + (y(2) - 2)^2\\\\ \\;\\;\\text{subject to} \\;\\; & - 1 - y(1)\\,y(2) \\leq 0, \\quad -1.5 + y(1) y(2) \\leq 0, \\\\ & 1\\,x \\;\\;+\\; \\;(\\,-1 \\;\\; 0 \\,)\\,y = 0, \\end{aligned} \\begin{aligned} & \\min_{x \\in \\mathbb{R},y \\in \\mathbb{R}^2} 2 \\,(x - 1)^2 + (y(2) - 2)^2\\\\ \\;\\;\\text{subject to} \\;\\; & - 1 - y(1)\\,y(2) \\leq 0, \\quad -1.5 + y(1) y(2) \\leq 0, \\\\ & 1\\,x \\;\\;+\\; \\;(\\,-1 \\;\\; 0 \\,)\\,y = 0, \\end{aligned} which is in the above form. In MATLAB code, this looks as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 % define local objective functions f1 = @( x ) 2 * ( x - 1 ) ^ 2 ; f2 = @( y ) ( y ( 2 ) - 2 ) ^ 2 ; % local nonlinear inequality constraints h1 = @( x ) []; h2 = @( y ) [ - 1 - y ( 1 ) * y ( 2 ) ; - 1.5 + y ( 1 ) * y ( 2 )]; % coupling matrices A1 = 1 ; A2 = [-1 0] ; % collect variables in sProb struct sProb . locFuns . ffi = { f1 , f2 }; sProb . locFuns . hhi = { h1 , h2 }; sProb . AA = { A1 , A2 }; That\u2019s all! Now we are ready to solve our problem with the run_ALADIN function. 1 sol_ALADIN = run_ALADINnew ( sProb ); If the option plot is true , ALADIN-M shows progress by the following plot while iterating. A sample plot is shown below. The resulting solver console output is shown next. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ======================================================== == This is ALADIN - alpha v0 .1 == ======================================================== QP solver : MA57 Local solver : ipopt Inner algorithm : none No termination criterion was specified . Consensus violation : 1.1368e-10 Maximum number of iterations reached . ----------------- ALADIN-M timing ------------------ t [ s ] % tot % iter Tot time : ...... : 2.5 Prob setup :.... : 0.1 2.3 Iter time : ..... : 2.4 97.6 ------ NLP time : ...... : 0.9 37.3 QP time : ....... : 0.0 1.5 Reg time : ...... : 0.0 0.4 Plot time : ..... : 1.4 56.2 ======================================================== For further examples checkout the /examples and the /test folder! How to install Clone https://github.com/alexe15/ALADIN.m and add /ALADIN.m to your MATLAB path. Requirements MATLAB CasADi MATLAB symbolic toolbox (only for examples) The current version is tested with MATLAB R2019b and CasADi 3.5.1.","title":"Getting Started"},{"location":"#aladin-alphaalpha","text":"ALADIN- \\alpha \\alpha is a rapid-prototyping toolbox for distributed and decentralized non-convex optimization. ALADIN- \\alpha \\alpha provides an implementation of the Augmented Lagrangian Alternating Direction Inexact Newton (ALADIN) algorithm and the Alternating Direction of Multipliers Method (ADMM) with a unified interface. Moreover, a bi-level ALADIN variant is included in ALADIN- \\alpha \\alpha allowing for decentralized non-convex optimization. Application examples from various fields highlight the broad applicability of ALADIN- \\alpha \\alpha . The toolbox is described in more detail here . ALADIN- \\alpha \\alpha solves problem of the form \\begin{aligned} &\\min_{x_1,\\dots,x_{n_s}} && \\sum_{i\\in \\mathcal{S}} f_i(x_i,p_i) \\\\ &\\;\\;\\text{subject to}&&g_{i}(x_i,p_i) = 0 \\quad &&\\mid \\kappa_i, &\\forall i \\in \\mathcal{R}, \\\\ &&&h_{i}(x_i,p_i) \\leq 0 \\quad \\;\\,&& \\mid \\gamma_i, &\\forall i \\in \\mathcal{R}, \\\\ &&&\\underline{x}_i \\leq x_i \\leq \\overline{x}_i\\;\\,&& \\mid\\eta_i, &\\forall i \\in \\mathcal{R}, \\\\ &&&\\sum_{i\\in \\mathcal{S}}A_i x_i=0\\;&&\\mid\\lambda. \\end{aligned} \\begin{aligned} &\\min_{x_1,\\dots,x_{n_s}} && \\sum_{i\\in \\mathcal{S}} f_i(x_i,p_i) \\\\ &\\;\\;\\text{subject to}&&g_{i}(x_i,p_i) = 0 \\quad &&\\mid \\kappa_i, &\\forall i \\in \\mathcal{R}, \\\\ &&&h_{i}(x_i,p_i) \\leq 0 \\quad \\;\\,&& \\mid \\gamma_i, &\\forall i \\in \\mathcal{R}, \\\\ &&&\\underline{x}_i \\leq x_i \\leq \\overline{x}_i\\;\\,&& \\mid\\eta_i, &\\forall i \\in \\mathcal{R}, \\\\ &&&\\sum_{i\\in \\mathcal{S}}A_i x_i=0\\;&&\\mid\\lambda. \\end{aligned} in a distributed fashion. Eearly-stage version of ALADIN- \\alpha \\alpha Note that ALADIN- \\alpha \\alpha is still in a prototypical phase of development.","title":"ALADIN-\\alpha\\alpha"},{"location":"#an-example","text":"Here\u2019s an example how to use ALADIN- \\alpha \\alpha . Let us consider an inequality-constrained non-convex problem \\begin{aligned} & \\min_{x \\in \\mathbb{R},y \\in \\mathbb{R}^2} 2 \\,(x - 1)^2 + (y(2) - 2)^2\\\\ \\;\\;\\text{subject to} \\;\\; & - 1 - y(1)\\,y(2) \\leq 0, \\quad -1.5 + y(1) y(2) \\leq 0, \\\\ & 1\\,x \\;\\;+\\; \\;(\\,-1 \\;\\; 0 \\,)\\,y = 0, \\end{aligned} \\begin{aligned} & \\min_{x \\in \\mathbb{R},y \\in \\mathbb{R}^2} 2 \\,(x - 1)^2 + (y(2) - 2)^2\\\\ \\;\\;\\text{subject to} \\;\\; & - 1 - y(1)\\,y(2) \\leq 0, \\quad -1.5 + y(1) y(2) \\leq 0, \\\\ & 1\\,x \\;\\;+\\; \\;(\\,-1 \\;\\; 0 \\,)\\,y = 0, \\end{aligned} which is in the above form. In MATLAB code, this looks as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 % define local objective functions f1 = @( x ) 2 * ( x - 1 ) ^ 2 ; f2 = @( y ) ( y ( 2 ) - 2 ) ^ 2 ; % local nonlinear inequality constraints h1 = @( x ) []; h2 = @( y ) [ - 1 - y ( 1 ) * y ( 2 ) ; - 1.5 + y ( 1 ) * y ( 2 )]; % coupling matrices A1 = 1 ; A2 = [-1 0] ; % collect variables in sProb struct sProb . locFuns . ffi = { f1 , f2 }; sProb . locFuns . hhi = { h1 , h2 }; sProb . AA = { A1 , A2 }; That\u2019s all! Now we are ready to solve our problem with the run_ALADIN function. 1 sol_ALADIN = run_ALADINnew ( sProb ); If the option plot is true , ALADIN-M shows progress by the following plot while iterating. A sample plot is shown below. The resulting solver console output is shown next. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ======================================================== == This is ALADIN - alpha v0 .1 == ======================================================== QP solver : MA57 Local solver : ipopt Inner algorithm : none No termination criterion was specified . Consensus violation : 1.1368e-10 Maximum number of iterations reached . ----------------- ALADIN-M timing ------------------ t [ s ] % tot % iter Tot time : ...... : 2.5 Prob setup :.... : 0.1 2.3 Iter time : ..... : 2.4 97.6 ------ NLP time : ...... : 0.9 37.3 QP time : ....... : 0.0 1.5 Reg time : ...... : 0.0 0.4 Plot time : ..... : 1.4 56.2 ======================================================== For further examples checkout the /examples and the /test folder!","title":"An example"},{"location":"#how-to-install","text":"Clone https://github.com/alexe15/ALADIN.m and add /ALADIN.m to your MATLAB path.","title":"How to install"},{"location":"#requirements","text":"MATLAB CasADi MATLAB symbolic toolbox (only for examples) The current version is tested with MATLAB R2019b and CasADi 3.5.1.","title":"Requirements"},{"location":"ALADIN/","text":"Algorithm description Initialization: Initial guess \\left (\\{z_i^0\\}_{i\\in \\mathcal{R}},\\lambda^0 \\right ) \\left (\\{z_i^0\\}_{i\\in \\mathcal{R}},\\lambda^0 \\right ) , choose \\Sigma_i \\succ 0,\\rho^0,\\mu^0,\\epsilon \\Sigma_i \\succ 0,\\rho^0,\\mu^0,\\epsilon . Repeat: Parallelizable Step: Solve for each i \\in \\mathcal{R} i \\in \\mathcal{R} \\begin{aligned} \\min_{x_i\\in [\\underline {x_i}, \\overline x_i]} &f_i(x_i,p_i) + (\\lambda^k)^\\top A_i x_i + \\frac{\\rho^k}{2}\\left\\|x_i-z_i^k\\right\\|_{\\Sigma_i}^2 \\;\\; \\\\ \\text{s.t.}\\quad & g_i(x_i,p_i) = 0, \\; \\;h_i(x_i,p_i)\\leq 0,\\; \\;\\; \\underline{x_i} \\leq x_i \\leq \\overline{x}_i. \\end{aligned} \\begin{aligned} \\min_{x_i\\in [\\underline {x_i}, \\overline x_i]} &f_i(x_i,p_i) + (\\lambda^k)^\\top A_i x_i + \\frac{\\rho^k}{2}\\left\\|x_i-z_i^k\\right\\|_{\\Sigma_i}^2 \\;\\; \\\\ \\text{s.t.}\\quad & g_i(x_i,p_i) = 0, \\; \\;h_i(x_i,p_i)\\leq 0,\\; \\;\\; \\underline{x_i} \\leq x_i \\leq \\overline{x}_i. \\end{aligned} Termination Criterion: If \\left\\|\\sum_{i\\in \\mathcal{R}}A_ix^k_i -b \\right\\|\\leq \\epsilon \\text{ and } \\left\\| x^k - z^k \\right \\|\\leq \\epsilon\\;, \\left\\|\\sum_{i\\in \\mathcal{R}}A_ix^k_i -b \\right\\|\\leq \\epsilon \\text{ and } \\left\\| x^k - z^k \\right \\|\\leq \\epsilon\\;, return x^\\star = x^k x^\\star = x^k . Sensitivity Evaluations: Compute and communicate local gradients g_i^k=\\nabla f_i(x_i^k,p_i) g_i^k=\\nabla f_i(x_i^k,p_i) , Hessian approximations 0 \\prec B_i^k \\approx \\nabla^2 \\{ f_i( x_i^k,p_i )+\\kappa_i^\\top h_i(x_i^k,p_i)\\} 0 \\prec B_i^k \\approx \\nabla^2 \\{ f_i( x_i^k,p_i )+\\kappa_i^\\top h_i(x_i^k,p_i)\\} and constraint Jacobians C^{k\\top }_i :=\\left [\\nabla g_i(x^k_i,p_i)^\\top\\; \\left (\\nabla \\tilde h_i(x^k_i,p_i) \\right )_{j\\in \\mathbb{A}^k}^\\top \\right ] C^{k\\top }_i :=\\left [\\nabla g_i(x^k_i,p_i)^\\top\\; \\left (\\nabla \\tilde h_i(x^k_i,p_i) \\right )_{j\\in \\mathbb{A}^k}^\\top \\right ] . Consensus Step: Solve the coordination QP \\begin{aligned} &\\underset{\\Delta x,s}{\\min}\\;\\;\\sum_{i\\in \\mathcal{R}}\\left\\{\\frac{1}{2}\\Delta x_i^\\top B^k_i\\Delta x_i + {g_i^k}^\\top \\Delta x_i\\right\\} + (\\lambda^k)^\\top s + \\frac{\\mu^k}{2}\\|s\\|^2_2 \\\\ & \\begin{aligned} \\text{subject to}\\; \\sum_{i\\in \\mathcal{R}}A_i(x^k_i+\\Delta x_i) &= s \\qquad |\\; \\lambda^{\\mathrm{QP} k},\\\\ C^k_i \\Delta x_i &= 0 \\qquad \\forall i\\in \\mathcal{R},\\\\ \\end{aligned} \\end{aligned} \\begin{aligned} &\\underset{\\Delta x,s}{\\min}\\;\\;\\sum_{i\\in \\mathcal{R}}\\left\\{\\frac{1}{2}\\Delta x_i^\\top B^k_i\\Delta x_i + {g_i^k}^\\top \\Delta x_i\\right\\} + (\\lambda^k)^\\top s + \\frac{\\mu^k}{2}\\|s\\|^2_2 \\\\ & \\begin{aligned} \\text{subject to}\\; \\sum_{i\\in \\mathcal{R}}A_i(x^k_i+\\Delta x_i) &= s \\qquad |\\; \\lambda^{\\mathrm{QP} k},\\\\ C^k_i \\Delta x_i &= 0 \\qquad \\forall i\\in \\mathcal{R},\\\\ \\end{aligned} \\end{aligned} yielding \\Delta x^k \\Delta x^k and \\lambda^{\\mathrm{QP}k} \\lambda^{\\mathrm{QP}k} as the solution to the above problem. Line Search: Update primal and dual variables by \\begin{aligned} z^{k+1}&\\leftarrow&z^k + \\alpha^k_1(x^k-z^k) + \\alpha_2^k\\Delta x^k \\qquad \\qquad \\lambda^{k+1}\\leftarrow\\lambda^k + \\alpha^k_3 (\\lambda^{\\mathrm{QP}k}-\\lambda^k), \\end{aligned} \\begin{aligned} z^{k+1}&\\leftarrow&z^k + \\alpha^k_1(x^k-z^k) + \\alpha_2^k\\Delta x^k \\qquad \\qquad \\lambda^{k+1}\\leftarrow\\lambda^k + \\alpha^k_3 (\\lambda^{\\mathrm{QP}k}-\\lambda^k), \\end{aligned} with \\alpha^k_1,\\alpha^k_2,\\alpha^k_3 \\alpha^k_1,\\alpha^k_2,\\alpha^k_3 from HFD16 .","title":"The Algorithm"},{"location":"ALADIN/#algorithm-description","text":"Initialization: Initial guess \\left (\\{z_i^0\\}_{i\\in \\mathcal{R}},\\lambda^0 \\right ) \\left (\\{z_i^0\\}_{i\\in \\mathcal{R}},\\lambda^0 \\right ) , choose \\Sigma_i \\succ 0,\\rho^0,\\mu^0,\\epsilon \\Sigma_i \\succ 0,\\rho^0,\\mu^0,\\epsilon . Repeat: Parallelizable Step: Solve for each i \\in \\mathcal{R} i \\in \\mathcal{R} \\begin{aligned} \\min_{x_i\\in [\\underline {x_i}, \\overline x_i]} &f_i(x_i,p_i) + (\\lambda^k)^\\top A_i x_i + \\frac{\\rho^k}{2}\\left\\|x_i-z_i^k\\right\\|_{\\Sigma_i}^2 \\;\\; \\\\ \\text{s.t.}\\quad & g_i(x_i,p_i) = 0, \\; \\;h_i(x_i,p_i)\\leq 0,\\; \\;\\; \\underline{x_i} \\leq x_i \\leq \\overline{x}_i. \\end{aligned} \\begin{aligned} \\min_{x_i\\in [\\underline {x_i}, \\overline x_i]} &f_i(x_i,p_i) + (\\lambda^k)^\\top A_i x_i + \\frac{\\rho^k}{2}\\left\\|x_i-z_i^k\\right\\|_{\\Sigma_i}^2 \\;\\; \\\\ \\text{s.t.}\\quad & g_i(x_i,p_i) = 0, \\; \\;h_i(x_i,p_i)\\leq 0,\\; \\;\\; \\underline{x_i} \\leq x_i \\leq \\overline{x}_i. \\end{aligned} Termination Criterion: If \\left\\|\\sum_{i\\in \\mathcal{R}}A_ix^k_i -b \\right\\|\\leq \\epsilon \\text{ and } \\left\\| x^k - z^k \\right \\|\\leq \\epsilon\\;, \\left\\|\\sum_{i\\in \\mathcal{R}}A_ix^k_i -b \\right\\|\\leq \\epsilon \\text{ and } \\left\\| x^k - z^k \\right \\|\\leq \\epsilon\\;, return x^\\star = x^k x^\\star = x^k . Sensitivity Evaluations: Compute and communicate local gradients g_i^k=\\nabla f_i(x_i^k,p_i) g_i^k=\\nabla f_i(x_i^k,p_i) , Hessian approximations 0 \\prec B_i^k \\approx \\nabla^2 \\{ f_i( x_i^k,p_i )+\\kappa_i^\\top h_i(x_i^k,p_i)\\} 0 \\prec B_i^k \\approx \\nabla^2 \\{ f_i( x_i^k,p_i )+\\kappa_i^\\top h_i(x_i^k,p_i)\\} and constraint Jacobians C^{k\\top }_i :=\\left [\\nabla g_i(x^k_i,p_i)^\\top\\; \\left (\\nabla \\tilde h_i(x^k_i,p_i) \\right )_{j\\in \\mathbb{A}^k}^\\top \\right ] C^{k\\top }_i :=\\left [\\nabla g_i(x^k_i,p_i)^\\top\\; \\left (\\nabla \\tilde h_i(x^k_i,p_i) \\right )_{j\\in \\mathbb{A}^k}^\\top \\right ] . Consensus Step: Solve the coordination QP \\begin{aligned} &\\underset{\\Delta x,s}{\\min}\\;\\;\\sum_{i\\in \\mathcal{R}}\\left\\{\\frac{1}{2}\\Delta x_i^\\top B^k_i\\Delta x_i + {g_i^k}^\\top \\Delta x_i\\right\\} + (\\lambda^k)^\\top s + \\frac{\\mu^k}{2}\\|s\\|^2_2 \\\\ & \\begin{aligned} \\text{subject to}\\; \\sum_{i\\in \\mathcal{R}}A_i(x^k_i+\\Delta x_i) &= s \\qquad |\\; \\lambda^{\\mathrm{QP} k},\\\\ C^k_i \\Delta x_i &= 0 \\qquad \\forall i\\in \\mathcal{R},\\\\ \\end{aligned} \\end{aligned} \\begin{aligned} &\\underset{\\Delta x,s}{\\min}\\;\\;\\sum_{i\\in \\mathcal{R}}\\left\\{\\frac{1}{2}\\Delta x_i^\\top B^k_i\\Delta x_i + {g_i^k}^\\top \\Delta x_i\\right\\} + (\\lambda^k)^\\top s + \\frac{\\mu^k}{2}\\|s\\|^2_2 \\\\ & \\begin{aligned} \\text{subject to}\\; \\sum_{i\\in \\mathcal{R}}A_i(x^k_i+\\Delta x_i) &= s \\qquad |\\; \\lambda^{\\mathrm{QP} k},\\\\ C^k_i \\Delta x_i &= 0 \\qquad \\forall i\\in \\mathcal{R},\\\\ \\end{aligned} \\end{aligned} yielding \\Delta x^k \\Delta x^k and \\lambda^{\\mathrm{QP}k} \\lambda^{\\mathrm{QP}k} as the solution to the above problem. Line Search: Update primal and dual variables by \\begin{aligned} z^{k+1}&\\leftarrow&z^k + \\alpha^k_1(x^k-z^k) + \\alpha_2^k\\Delta x^k \\qquad \\qquad \\lambda^{k+1}\\leftarrow\\lambda^k + \\alpha^k_3 (\\lambda^{\\mathrm{QP}k}-\\lambda^k), \\end{aligned} \\begin{aligned} z^{k+1}&\\leftarrow&z^k + \\alpha^k_1(x^k-z^k) + \\alpha_2^k\\Delta x^k \\qquad \\qquad \\lambda^{k+1}\\leftarrow\\lambda^k + \\alpha^k_3 (\\lambda^{\\mathrm{QP}k}-\\lambda^k), \\end{aligned} with \\alpha^k_1,\\alpha^k_2,\\alpha^k_3 \\alpha^k_1,\\alpha^k_2,\\alpha^k_3 from HFD16 .","title":"Algorithm description"},{"location":"ParallelExample/","text":"Using ALADIN- \\alpha \\alpha with parfor Here we consider a sensor network localization problem from the SIAM ALADIN paper . We illustrate, how the parfor option of ALADIN- \\alpha \\alpha can be used for parallel execution. For this example the MATLAB parallel computing toolbox is required. Problem setup Let N N be the number of sensors and let \\chi_i =(x_i,y_i)^\\top \\in \\mathbb{R}^2 \\chi_i =(x_i,y_i)^\\top \\in \\mathbb{R}^2 be the unknown position of the i i th sensor, let \\eta_i \\eta_i be its estimated position, let \\xi_i \\xi_i be the position of senosor i+1 i+1 estimated by sensor i i let \\bar{\\eta_i} \\bar{\\eta_i} be the estimated distance between sensor i i and its neighbors. The measurement error is given by \\eta_i - \\chi_i \\eta_i - \\chi_i and is assumed to be Gaussian distributed with variance \\sigma_i^2 I_{2 \\times 2} \\sigma_i^2 I_{2 \\times 2} . We further denote the measured distance between sensor i i and sensor i + 1 i + 1 by \\bar{\\eta_i} \\bar{\\eta_i} . If we define the decision variable as x_i = (\\chi_i^\\top, \\xi_i^\\top)^\\top \\in \\mathbb{R}^4 x_i = (\\chi_i^\\top, \\xi_i^\\top)^\\top \\in \\mathbb{R}^4 then we obtain the overall problem \\begin{aligned} \\displaystyle \\min_x &\\sum_{i = 1}^N f_i(x_i)\\quad \\\\ \\text{ s.t. } &\\quad h_i(x_i) \\leq 0 \\quad \\forall i \\in \\{1, \\dots, N\\}\\\\ & \\quad\\xi_i = \\chi_{i+1} \\quad \\text{ }\\text{ }\\forall i \\in \\{1, \\dots, N\\} \\end{aligned} \\begin{aligned} \\displaystyle \\min_x &\\sum_{i = 1}^N f_i(x_i)\\quad \\\\ \\text{ s.t. } &\\quad h_i(x_i) \\leq 0 \\quad \\forall i \\in \\{1, \\dots, N\\}\\\\ & \\quad\\xi_i = \\chi_{i+1} \\quad \\text{ }\\text{ }\\forall i \\in \\{1, \\dots, N\\} \\end{aligned} with f_i(x_i)=\\frac{1}{4\\sigma_i^2}\\vert\\vert\\chi_i - \\eta_i\\vert\\vert_2^2 + \\frac{1}{4\\sigma_{i+1}^2}\\vert\\vert\\xi_i - \\eta_{i+1}\\vert\\vert_2^2 + \\frac{1}{2\\sigma_{i+1}^2}(\\vert\\vert\\chi_i - \\xi_i\\vert\\vert_2^2 - \\bar{\\eta}_i)^2 f_i(x_i)=\\frac{1}{4\\sigma_i^2}\\vert\\vert\\chi_i - \\eta_i\\vert\\vert_2^2 + \\frac{1}{4\\sigma_{i+1}^2}\\vert\\vert\\xi_i - \\eta_{i+1}\\vert\\vert_2^2 + \\frac{1}{2\\sigma_{i+1}^2}(\\vert\\vert\\chi_i - \\xi_i\\vert\\vert_2^2 - \\bar{\\eta}_i)^2 and h_i(x_i) = (\\vert\\vert\\chi_i - \\xi_i\\vert\\vert_2 - \\bar{\\eta_i})^2 - \\bar{\\sigma_i}^2 h_i(x_i) = (\\vert\\vert\\chi_i - \\xi_i\\vert\\vert_2 - \\bar{\\eta_i})^2 - \\bar{\\sigma_i}^2 . Implementation For the implementaiton, firstly the problem needs to be defined in a way that is compatible to ALADIN- \\alpha \\alpha . The definitions of variables and functions are executed in separate functions: for the computation of \\eta_i \\eta_i and \\bar{\\eta_i} \\bar{\\eta_i} the sensors are assumed to be equidistantly located in a circle, i.e. $$ \\chi_i = \\left(N \\cos (\\frac{2i\\pi}{N}), N \\sin (\\frac{2i\\pi}{N})\\right). $$ The measuremnt errors are assumed to be normal distributed with variance sigma. The neighbour of sensor n n is assumed to be sensor 1 1 , thus we define \\eta_{n+1} = \\eta_1 \\eta_{n+1} = \\eta_1 . So we get: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 function [eta,eta_bar] = getEta ( N, d, sigma ) % returns by sensor estimated initial position and distance to its neighbours eta = zeros(d, N + 1) ; eta_bar = zeros ( 1 , N ); for i = 1 : N eta ( :, i ) = [ N * cos ( 2 * i * pi / N ) + normrnd ( 0 , sigma ) ; ... N * sin ( 2 * i * pi / N ) + normrnd ( 0 , sigma )]; eta_bar ( i ) = 2 * N * sin ( pi / N ) + normrnd ( 0 , sigma ); end eta(:, N + 1) = eta(:, 1) ; end Implementing f f and h h as above we obtain 1 2 3 4 5 6 7 8 9 10 function [F] = getObjective ( N, y, eta, eta_bar, sigma ) F = zeros ( N , 1 ); F = sym ( F ); F (:) = 1 / ( 4 * sigma ^ 2 ) * (( y ( 1 , :) - eta ( 1 , 1 : N )) .^ 2 + ( y ( 2 , :) - eta ( 2 , 1 : N )) .^ 2 ) ... + 1 / ( 4 * sigma ^ 2 ) * (( y ( 3 , :) - eta ( 1 , 2 : end )) .^ 2 + ( y ( 4 , :) - eta ( 2 , 2 : end )) .^ 2 ) ... + 1 / ( 2 * sigma ^ 2 ) * ( sqrt (( y ( 1 , :) - y ( 3 , :)) .^ 2 + ( y ( 2 , :) - y ( 4 , :)) .^ 2 ) - eta_bar (:) ' ) .^ 2 ; end and 1 2 3 4 5 6 7 8 function [H] = getInequalityConstr ( N, y, eta_bar ) H = zeros ( N , 1 ); H = sym ( H ); H (:) = ( sqrt (( y ( 1 , :) - y ( 3 , :)) .^ 2 + ( y ( 2 , :) - y ( 4 , :)) .^ 2 ) - eta_bar (:) ' ) .^ 2 ; end The coupling condition \\xi_i = \\chi_{i+1} \\xi_i = \\chi_{i+1} can be formulated as \\sum A_ix_i = 0 \\sum A_ix_i = 0 with A_1 = \\begin{pmatrix} 0 & I \\\\ 0 & 0 \\\\ 0 & 0 \\\\ \\vdots & \\vdots \\\\ 0 & 0 \\\\ -I & 0 \\end{pmatrix}, \\; A_2 = \\begin{pmatrix} -I & 0 \\\\ 0 & I \\\\ 0 & 0 \\\\ \\vdots & \\vdots \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\; A_3 = \\begin{pmatrix} 0 & 0 \\\\ -I & 0 \\\\ 0 & I \\\\ \\vdots & \\vdots \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\; \\cdots \\;, \\; A_N = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 0 & 0 \\\\ \\vdots & \\vdots \\\\ -I & 0 \\\\ 0 & I \\end{pmatrix} A_1 = \\begin{pmatrix} 0 & I \\\\ 0 & 0 \\\\ 0 & 0 \\\\ \\vdots & \\vdots \\\\ 0 & 0 \\\\ -I & 0 \\end{pmatrix}, \\; A_2 = \\begin{pmatrix} -I & 0 \\\\ 0 & I \\\\ 0 & 0 \\\\ \\vdots & \\vdots \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\; A_3 = \\begin{pmatrix} 0 & 0 \\\\ -I & 0 \\\\ 0 & I \\\\ \\vdots & \\vdots \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\; \\cdots \\;, \\; A_N = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 0 & 0 \\\\ \\vdots & \\vdots \\\\ -I & 0 \\\\ 0 & I \\end{pmatrix} while setting I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} In Matlab this can be implemented as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 function [AA] = getCouplingMatrix ( N, n ) I = [ 1 , 0 ; 0 , 1 ]; A0 = zeros ( 2 * N , n ); A1 = A0 ; A1 ( 1 : 2 , 3 : 4 ) = I ; A1 ( 2 * N - 1 : 2 * N , 1 : 2 ) = - I ; AA ( 1 ) = mat2cell ( A_1 , 2 * N , n ); for i = 2 : 1 : N A_i = A0 ; A_i ( 2 * ( i - 2 ) + 1 : 2 * ( i - 2 ) + 2 , 1 : 2 ) = - I ; A_i ( 2 * i - 1 : 2 * i , 3 : 4 ) = I ; AA ( i ) = mat2cell ( A_i , 2 * N , n ); end end A start vector can be defined similarly to the estimated positions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 function [zz0] = getStartValue ( N, sigma ) initial_position = zeros ( 2 , N ); for i = 1 : N initial_position ( 1 , i ) = N * cos ( 2 * i * pi / N ) + normrnd ( 0 , sigma ); initial_position ( 2 , i ) = N * sin ( 2 * i * pi / N ) + normrnd ( 0 , sigma ); end zz0 = cell(1, N) ; for i = 1 : N-1 zz0 ( i ) = {[ initial_position (:, i ); initial_position (:, i + 1 )]}; end zz0(N) = {[initial_position(:, N) ; initial_position (:, 1 )]}; end such that the overall problem can be set up with the following function: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 function [sProb ] = setupSolver ( N, sigma ) % general setup n = 4 ; % dimension of design variables d = 2 ; % dimension of coordinate system % initialization of variables y = sym ( 'y%d%d' , [ N n ], 'real' ); y = y ' ; [ eta , eta_bar ] = getEta ( N , d , sigma ); % definition of estimated initial positions F = getObjective(N, y, eta, eta_bar, sigma) ; % definition of objective functions H = getInequalityConstr(N, y, eta_bar) ; % definition of inequality constraint AA = getCouplingMatrix(N, n) ; % definition of coupling matrix zz0 = getStartValue ( N , sigma ); % definition of start value for optimization %% setting up solver % set search area sProb . llbx = cell ( 1 , N ); sProb . uubx = cell ( 1 , N ); for i = 1 : N sProb . llbx ( i ) = mat2cell ([ - inf ; - inf ; - inf ; - inf ], 4 , 1 ); sProb . uubx ( i ) = mat2cell ([ inf ; inf ; inf ; inf ], 4 , 1 ); end % handover of functions sProb . locFuns . ffi = cell ( 1 , N ); sProb . locFuns . hhi = cell ( 1 , N ); for i = 1 : N sProb . locFuns . ffi ( i ) = { matlabFunction ( F ( i ), 'Vars' , { y (:, i )})} ; sProb . locFuns . hhi ( i ) = { matlabFunction ( H ( i ), 'Vars' , { y (:, i )})} ; end sProb.AA = AA ; sProb . zz0 = zz0 ; Runtime Analysis For the runtime analysis, the idea is to \u0155un the sensor network localization problem with varying number of sensors both with a decentral and a central optimization step. To do so, firstly a vector with a number of sensors is needed and secondly a vector with variances. Then, the time needed for the decentral and the central optimization is measured and can be plotted. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 N = [ 5 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 50 , 60 , 70 , 80 , 90 , 100 ]; sigma = [ 0.5 , 1 , 1.5 , 2 , 2.5 , 2.5 , 2.5 , 2.5 , 2.5 , 2.5 , 2.5 , 2.5 , 2.5 , 2.5 ]; time = zeros ( 2 , length ( N )); for i = 1 : length(N) sProb = setupSolver ( N ( i ), sigma ( i )); opts . parfor = 'true' time_parfor = tic ; sol = run_ALADINnew ( sProb , opts ); time ( 1 , i ) = toc ( time_parfor ); time_for = tic ; sol = run_ALADINnew ( sProb , opts ); time ( 2 , i ) = toc ( time_for ); end figure plot ( N , time ( 1 , :)) title ( 'runtime analysis' ) hold on plot ( N , time ( 2 , :)) hold off legend ( 'decentral optimization' , 'central optimization' ) Result The runtime result can be obtained from the following figure. Thus, for the case of the senor network localization problem a significant runtime improvement can be observed when the parfor option is set. Nonetheless it needs to be mentioned that an improvement on the runtime cannot always be achieved for every problem setup using the parfor option. In general, parfor is useful when the number of local optimization problems is large and the time for solving each of the local optimization problems is relatively long. For details, see decide when to use parfor","title":"Parallel Computing"},{"location":"ParallelExample/#using-aladin-alphaalpha-with-parfor","text":"Here we consider a sensor network localization problem from the SIAM ALADIN paper . We illustrate, how the parfor option of ALADIN- \\alpha \\alpha can be used for parallel execution. For this example the MATLAB parallel computing toolbox is required.","title":"Using ALADIN-\\alpha\\alpha with parfor"},{"location":"ParallelExample/#problem-setup","text":"Let N N be the number of sensors and let \\chi_i =(x_i,y_i)^\\top \\in \\mathbb{R}^2 \\chi_i =(x_i,y_i)^\\top \\in \\mathbb{R}^2 be the unknown position of the i i th sensor, let \\eta_i \\eta_i be its estimated position, let \\xi_i \\xi_i be the position of senosor i+1 i+1 estimated by sensor i i let \\bar{\\eta_i} \\bar{\\eta_i} be the estimated distance between sensor i i and its neighbors. The measurement error is given by \\eta_i - \\chi_i \\eta_i - \\chi_i and is assumed to be Gaussian distributed with variance \\sigma_i^2 I_{2 \\times 2} \\sigma_i^2 I_{2 \\times 2} . We further denote the measured distance between sensor i i and sensor i + 1 i + 1 by \\bar{\\eta_i} \\bar{\\eta_i} . If we define the decision variable as x_i = (\\chi_i^\\top, \\xi_i^\\top)^\\top \\in \\mathbb{R}^4 x_i = (\\chi_i^\\top, \\xi_i^\\top)^\\top \\in \\mathbb{R}^4 then we obtain the overall problem \\begin{aligned} \\displaystyle \\min_x &\\sum_{i = 1}^N f_i(x_i)\\quad \\\\ \\text{ s.t. } &\\quad h_i(x_i) \\leq 0 \\quad \\forall i \\in \\{1, \\dots, N\\}\\\\ & \\quad\\xi_i = \\chi_{i+1} \\quad \\text{ }\\text{ }\\forall i \\in \\{1, \\dots, N\\} \\end{aligned} \\begin{aligned} \\displaystyle \\min_x &\\sum_{i = 1}^N f_i(x_i)\\quad \\\\ \\text{ s.t. } &\\quad h_i(x_i) \\leq 0 \\quad \\forall i \\in \\{1, \\dots, N\\}\\\\ & \\quad\\xi_i = \\chi_{i+1} \\quad \\text{ }\\text{ }\\forall i \\in \\{1, \\dots, N\\} \\end{aligned} with f_i(x_i)=\\frac{1}{4\\sigma_i^2}\\vert\\vert\\chi_i - \\eta_i\\vert\\vert_2^2 + \\frac{1}{4\\sigma_{i+1}^2}\\vert\\vert\\xi_i - \\eta_{i+1}\\vert\\vert_2^2 + \\frac{1}{2\\sigma_{i+1}^2}(\\vert\\vert\\chi_i - \\xi_i\\vert\\vert_2^2 - \\bar{\\eta}_i)^2 f_i(x_i)=\\frac{1}{4\\sigma_i^2}\\vert\\vert\\chi_i - \\eta_i\\vert\\vert_2^2 + \\frac{1}{4\\sigma_{i+1}^2}\\vert\\vert\\xi_i - \\eta_{i+1}\\vert\\vert_2^2 + \\frac{1}{2\\sigma_{i+1}^2}(\\vert\\vert\\chi_i - \\xi_i\\vert\\vert_2^2 - \\bar{\\eta}_i)^2 and h_i(x_i) = (\\vert\\vert\\chi_i - \\xi_i\\vert\\vert_2 - \\bar{\\eta_i})^2 - \\bar{\\sigma_i}^2 h_i(x_i) = (\\vert\\vert\\chi_i - \\xi_i\\vert\\vert_2 - \\bar{\\eta_i})^2 - \\bar{\\sigma_i}^2 .","title":"Problem setup"},{"location":"ParallelExample/#implementation","text":"For the implementaiton, firstly the problem needs to be defined in a way that is compatible to ALADIN- \\alpha \\alpha . The definitions of variables and functions are executed in separate functions: for the computation of \\eta_i \\eta_i and \\bar{\\eta_i} \\bar{\\eta_i} the sensors are assumed to be equidistantly located in a circle, i.e. $$ \\chi_i = \\left(N \\cos (\\frac{2i\\pi}{N}), N \\sin (\\frac{2i\\pi}{N})\\right). $$ The measuremnt errors are assumed to be normal distributed with variance sigma. The neighbour of sensor n n is assumed to be sensor 1 1 , thus we define \\eta_{n+1} = \\eta_1 \\eta_{n+1} = \\eta_1 . So we get: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 function [eta,eta_bar] = getEta ( N, d, sigma ) % returns by sensor estimated initial position and distance to its neighbours eta = zeros(d, N + 1) ; eta_bar = zeros ( 1 , N ); for i = 1 : N eta ( :, i ) = [ N * cos ( 2 * i * pi / N ) + normrnd ( 0 , sigma ) ; ... N * sin ( 2 * i * pi / N ) + normrnd ( 0 , sigma )]; eta_bar ( i ) = 2 * N * sin ( pi / N ) + normrnd ( 0 , sigma ); end eta(:, N + 1) = eta(:, 1) ; end Implementing f f and h h as above we obtain 1 2 3 4 5 6 7 8 9 10 function [F] = getObjective ( N, y, eta, eta_bar, sigma ) F = zeros ( N , 1 ); F = sym ( F ); F (:) = 1 / ( 4 * sigma ^ 2 ) * (( y ( 1 , :) - eta ( 1 , 1 : N )) .^ 2 + ( y ( 2 , :) - eta ( 2 , 1 : N )) .^ 2 ) ... + 1 / ( 4 * sigma ^ 2 ) * (( y ( 3 , :) - eta ( 1 , 2 : end )) .^ 2 + ( y ( 4 , :) - eta ( 2 , 2 : end )) .^ 2 ) ... + 1 / ( 2 * sigma ^ 2 ) * ( sqrt (( y ( 1 , :) - y ( 3 , :)) .^ 2 + ( y ( 2 , :) - y ( 4 , :)) .^ 2 ) - eta_bar (:) ' ) .^ 2 ; end and 1 2 3 4 5 6 7 8 function [H] = getInequalityConstr ( N, y, eta_bar ) H = zeros ( N , 1 ); H = sym ( H ); H (:) = ( sqrt (( y ( 1 , :) - y ( 3 , :)) .^ 2 + ( y ( 2 , :) - y ( 4 , :)) .^ 2 ) - eta_bar (:) ' ) .^ 2 ; end The coupling condition \\xi_i = \\chi_{i+1} \\xi_i = \\chi_{i+1} can be formulated as \\sum A_ix_i = 0 \\sum A_ix_i = 0 with A_1 = \\begin{pmatrix} 0 & I \\\\ 0 & 0 \\\\ 0 & 0 \\\\ \\vdots & \\vdots \\\\ 0 & 0 \\\\ -I & 0 \\end{pmatrix}, \\; A_2 = \\begin{pmatrix} -I & 0 \\\\ 0 & I \\\\ 0 & 0 \\\\ \\vdots & \\vdots \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\; A_3 = \\begin{pmatrix} 0 & 0 \\\\ -I & 0 \\\\ 0 & I \\\\ \\vdots & \\vdots \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\; \\cdots \\;, \\; A_N = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 0 & 0 \\\\ \\vdots & \\vdots \\\\ -I & 0 \\\\ 0 & I \\end{pmatrix} A_1 = \\begin{pmatrix} 0 & I \\\\ 0 & 0 \\\\ 0 & 0 \\\\ \\vdots & \\vdots \\\\ 0 & 0 \\\\ -I & 0 \\end{pmatrix}, \\; A_2 = \\begin{pmatrix} -I & 0 \\\\ 0 & I \\\\ 0 & 0 \\\\ \\vdots & \\vdots \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\; A_3 = \\begin{pmatrix} 0 & 0 \\\\ -I & 0 \\\\ 0 & I \\\\ \\vdots & \\vdots \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\; \\cdots \\;, \\; A_N = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 0 & 0 \\\\ \\vdots & \\vdots \\\\ -I & 0 \\\\ 0 & I \\end{pmatrix} while setting I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} In Matlab this can be implemented as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 function [AA] = getCouplingMatrix ( N, n ) I = [ 1 , 0 ; 0 , 1 ]; A0 = zeros ( 2 * N , n ); A1 = A0 ; A1 ( 1 : 2 , 3 : 4 ) = I ; A1 ( 2 * N - 1 : 2 * N , 1 : 2 ) = - I ; AA ( 1 ) = mat2cell ( A_1 , 2 * N , n ); for i = 2 : 1 : N A_i = A0 ; A_i ( 2 * ( i - 2 ) + 1 : 2 * ( i - 2 ) + 2 , 1 : 2 ) = - I ; A_i ( 2 * i - 1 : 2 * i , 3 : 4 ) = I ; AA ( i ) = mat2cell ( A_i , 2 * N , n ); end end A start vector can be defined similarly to the estimated positions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 function [zz0] = getStartValue ( N, sigma ) initial_position = zeros ( 2 , N ); for i = 1 : N initial_position ( 1 , i ) = N * cos ( 2 * i * pi / N ) + normrnd ( 0 , sigma ); initial_position ( 2 , i ) = N * sin ( 2 * i * pi / N ) + normrnd ( 0 , sigma ); end zz0 = cell(1, N) ; for i = 1 : N-1 zz0 ( i ) = {[ initial_position (:, i ); initial_position (:, i + 1 )]}; end zz0(N) = {[initial_position(:, N) ; initial_position (:, 1 )]}; end such that the overall problem can be set up with the following function: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 function [sProb ] = setupSolver ( N, sigma ) % general setup n = 4 ; % dimension of design variables d = 2 ; % dimension of coordinate system % initialization of variables y = sym ( 'y%d%d' , [ N n ], 'real' ); y = y ' ; [ eta , eta_bar ] = getEta ( N , d , sigma ); % definition of estimated initial positions F = getObjective(N, y, eta, eta_bar, sigma) ; % definition of objective functions H = getInequalityConstr(N, y, eta_bar) ; % definition of inequality constraint AA = getCouplingMatrix(N, n) ; % definition of coupling matrix zz0 = getStartValue ( N , sigma ); % definition of start value for optimization %% setting up solver % set search area sProb . llbx = cell ( 1 , N ); sProb . uubx = cell ( 1 , N ); for i = 1 : N sProb . llbx ( i ) = mat2cell ([ - inf ; - inf ; - inf ; - inf ], 4 , 1 ); sProb . uubx ( i ) = mat2cell ([ inf ; inf ; inf ; inf ], 4 , 1 ); end % handover of functions sProb . locFuns . ffi = cell ( 1 , N ); sProb . locFuns . hhi = cell ( 1 , N ); for i = 1 : N sProb . locFuns . ffi ( i ) = { matlabFunction ( F ( i ), 'Vars' , { y (:, i )})} ; sProb . locFuns . hhi ( i ) = { matlabFunction ( H ( i ), 'Vars' , { y (:, i )})} ; end sProb.AA = AA ; sProb . zz0 = zz0 ;","title":"Implementation"},{"location":"ParallelExample/#runtime-analysis","text":"For the runtime analysis, the idea is to \u0155un the sensor network localization problem with varying number of sensors both with a decentral and a central optimization step. To do so, firstly a vector with a number of sensors is needed and secondly a vector with variances. Then, the time needed for the decentral and the central optimization is measured and can be plotted. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 N = [ 5 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 50 , 60 , 70 , 80 , 90 , 100 ]; sigma = [ 0.5 , 1 , 1.5 , 2 , 2.5 , 2.5 , 2.5 , 2.5 , 2.5 , 2.5 , 2.5 , 2.5 , 2.5 , 2.5 ]; time = zeros ( 2 , length ( N )); for i = 1 : length(N) sProb = setupSolver ( N ( i ), sigma ( i )); opts . parfor = 'true' time_parfor = tic ; sol = run_ALADINnew ( sProb , opts ); time ( 1 , i ) = toc ( time_parfor ); time_for = tic ; sol = run_ALADINnew ( sProb , opts ); time ( 2 , i ) = toc ( time_for ); end figure plot ( N , time ( 1 , :)) title ( 'runtime analysis' ) hold on plot ( N , time ( 2 , :)) hold off legend ( 'decentral optimization' , 'central optimization' )","title":"Runtime Analysis"},{"location":"ParallelExample/#result","text":"The runtime result can be obtained from the following figure. Thus, for the case of the senor network localization problem a significant runtime improvement can be observed when the parfor option is set. Nonetheless it needs to be mentioned that an improvement on the runtime cannot always be achieved for every problem setup using the parfor option. In general, parfor is useful when the number of local optimization problems is large and the time for solving each of the local optimization problems is relatively long. For details, see decide when to use parfor","title":"Result"},{"location":"codeStruct/","text":"Code structure Solver input and output The following figure summarizes the input and output structures sProb and res of ALADIN- \\alpha \\alpha . ALADIN- \\alpha \\alpha is started by running the function \\texttt{run\\_ALADIN} \\texttt{run\\_ALADIN} . As an input, the user specifies structs \\texttt{sProb} \\texttt{sProb} and \\texttt{opts} \\texttt{opts} , where the latter is optional. The struct \\texttt{sProb} \\texttt{sProb} contains the problem formulation. The struct \\texttt{opts} \\texttt{opts} specifies solver options. Possible options are described here . \\texttt{sProb} \\texttt{sProb} contains five fields namely the objective and constraint functions, which are stored in 1\\times \\texttt{N} 1\\times \\texttt{N} cells and collected in the struct \\texttt{locFuns} \\texttt{locFuns} , the coupling matrices \\texttt{AA} \\texttt{AA} , upper and lower bounds \\texttt{uubx} \\texttt{uubx} and \\texttt{llbx} \\texttt{llbx} , the start value \\texttt{zz0} \\texttt{zz0} which all are itself 1 \\times \\texttt{N} 1 \\times \\texttt{N} cells ( \\texttt{N} \\texttt{N} is the number of subsystems). The function \\texttt{run\\_ALADIN} \\texttt{run\\_ALADIN} returns a struct \\texttt{res} \\texttt{res} containing a cell \\texttt{xxOpt} \\texttt{xxOpt} with the optimal solution, a field \\texttt{lamOpt} \\texttt{lamOpt} with the optimal Lagrange multipliers \\lambda \\lambda , a struct \\texttt{iter} \\texttt{iter} that loggs iteration results and a struct \\texttt{timers} \\texttt{timers} that loggs the times needed for the different iterations. Solver structure The code structure is summarized in the following figure. In a preprocessing step, the the function \\texttt{checkInput} \\texttt{checkInput} verifies whether \\texttt{sProb} \\texttt{sProb} and \\texttt{opts} \\texttt{opts} contain a valid problem formulation and valid inputs. The function \\texttt{setDefaultOpts} \\texttt{setDefaultOpts} supplements missing options in \\texttt{opts} \\texttt{opts} with default values. In the following step, the local NLP solvers and sensitivities are constructed in \\texttt{createLocSolAndSens} \\texttt{createLocSolAndSens} . Thereby, the MATLAB functions given in \\texttt{locFuns} \\texttt{locFuns} are converted to CasADI functions. \\texttt{iterateAL} \\texttt{iterateAL} contains the main ALADIN loop. The loop consists of five steps. In the \\texttt{parallelStep} \\texttt{parallelStep} , three of them are executed in parallel, namely solving the decoupled NLPs, evaluating the sensitivities and approximating the Hessians. Step four and five solve the coordination QP and compute the primal/dual step. If a certain termination criterion is fulfilled, postprocessing starts displaying the needed number of iterations and displaying timing information.","title":"Code Structure"},{"location":"codeStruct/#code-structure","text":"","title":"Code structure"},{"location":"codeStruct/#solver-input-and-output","text":"The following figure summarizes the input and output structures sProb and res of ALADIN- \\alpha \\alpha . ALADIN- \\alpha \\alpha is started by running the function \\texttt{run\\_ALADIN} \\texttt{run\\_ALADIN} . As an input, the user specifies structs \\texttt{sProb} \\texttt{sProb} and \\texttt{opts} \\texttt{opts} , where the latter is optional. The struct \\texttt{sProb} \\texttt{sProb} contains the problem formulation. The struct \\texttt{opts} \\texttt{opts} specifies solver options. Possible options are described here . \\texttt{sProb} \\texttt{sProb} contains five fields namely the objective and constraint functions, which are stored in 1\\times \\texttt{N} 1\\times \\texttt{N} cells and collected in the struct \\texttt{locFuns} \\texttt{locFuns} , the coupling matrices \\texttt{AA} \\texttt{AA} , upper and lower bounds \\texttt{uubx} \\texttt{uubx} and \\texttt{llbx} \\texttt{llbx} , the start value \\texttt{zz0} \\texttt{zz0} which all are itself 1 \\times \\texttt{N} 1 \\times \\texttt{N} cells ( \\texttt{N} \\texttt{N} is the number of subsystems). The function \\texttt{run\\_ALADIN} \\texttt{run\\_ALADIN} returns a struct \\texttt{res} \\texttt{res} containing a cell \\texttt{xxOpt} \\texttt{xxOpt} with the optimal solution, a field \\texttt{lamOpt} \\texttt{lamOpt} with the optimal Lagrange multipliers \\lambda \\lambda , a struct \\texttt{iter} \\texttt{iter} that loggs iteration results and a struct \\texttt{timers} \\texttt{timers} that loggs the times needed for the different iterations.","title":"Solver input and output"},{"location":"codeStruct/#solver-structure","text":"The code structure is summarized in the following figure. In a preprocessing step, the the function \\texttt{checkInput} \\texttt{checkInput} verifies whether \\texttt{sProb} \\texttt{sProb} and \\texttt{opts} \\texttt{opts} contain a valid problem formulation and valid inputs. The function \\texttt{setDefaultOpts} \\texttt{setDefaultOpts} supplements missing options in \\texttt{opts} \\texttt{opts} with default values. In the following step, the local NLP solvers and sensitivities are constructed in \\texttt{createLocSolAndSens} \\texttt{createLocSolAndSens} . Thereby, the MATLAB functions given in \\texttt{locFuns} \\texttt{locFuns} are converted to CasADI functions. \\texttt{iterateAL} \\texttt{iterateAL} contains the main ALADIN loop. The loop consists of five steps. In the \\texttt{parallelStep} \\texttt{parallelStep} , three of them are executed in parallel, namely solving the decoupled NLPs, evaluating the sensitivities and approximating the Hessians. Step four and five solve the coordination QP and compute the primal/dual step. If a certain termination criterion is fulfilled, postprocessing starts displaying the needed number of iterations and displaying timing information.","title":"Solver structure"},{"location":"furtherReading/","text":"References ALADIN- \\alpha \\alpha is descibed in ArXiv . Different versions of ALADIN are implemented in ALADIN- \\alpha \\alpha . Algorithmic details are described in the publications below. Toolbox [1] Engelmann, A., Jiang, Y., Benner, H., Ou, R., Houska, B., & Faulwasser, T. (2020). ALADIN-$\\alpha $\u2013An open-source MATLAB toolbox for distributed non-convex optimization. arXiv preprint arXiv:2006.01866. Algorithmic Details [2] Houska, B., Frasch, J., & Diehl, M. (2016). An augmented Lagrangian based algorithm for distributed nonconvex optimization. SIAM Journal on Optimization, 26(2), 1101-1127. [3] Engelmann, A., Jiang, Y., Houska, B., & Faulwasser, T. (2019). Decomposition of non-convex optimization via bi-level distributed ALADIN. arXiv preprint arXiv:1903.11280. Application to power systems [4] Engelmann, A., Jiang, Y., M\u00fchlpfordt, T., Houska, B., & Faulwasser, T. (2018). Toward distributed OPF using ALADIN. IEEE Transactions on Power Systems, 34(1), 584-594. [5] Engelmann, A., M\u00fchlpfordt, T., Jiang, Y., Houska, B., & Faulwasser, T. (2017). Distributed AC optimal power flow using ALADIN. IFAC-PapersOnLine, 50(1), 5536-5541. [6] Du, X., Engelmann, A., Jiang, Y., Faulwasser, T., & Houska, B. (2019). Distributed State Estimation for AC Power Systems using Gauss-Newton ALADIN. arXiv preprint arXiv:1903.08956. Application to Traffic engineering Application to Optimal control","title":"Publications"},{"location":"furtherReading/#references","text":"ALADIN- \\alpha \\alpha is descibed in ArXiv . Different versions of ALADIN are implemented in ALADIN- \\alpha \\alpha . Algorithmic details are described in the publications below.","title":"References"},{"location":"furtherReading/#toolbox","text":"[1] Engelmann, A., Jiang, Y., Benner, H., Ou, R., Houska, B., & Faulwasser, T. (2020). ALADIN-$\\alpha $\u2013An open-source MATLAB toolbox for distributed non-convex optimization. arXiv preprint arXiv:2006.01866.","title":"Toolbox"},{"location":"furtherReading/#algorithmic-details","text":"[2] Houska, B., Frasch, J., & Diehl, M. (2016). An augmented Lagrangian based algorithm for distributed nonconvex optimization. SIAM Journal on Optimization, 26(2), 1101-1127. [3] Engelmann, A., Jiang, Y., Houska, B., & Faulwasser, T. (2019). Decomposition of non-convex optimization via bi-level distributed ALADIN. arXiv preprint arXiv:1903.11280.","title":"Algorithmic Details"},{"location":"furtherReading/#application-to-power-systems","text":"[4] Engelmann, A., Jiang, Y., M\u00fchlpfordt, T., Houska, B., & Faulwasser, T. (2018). Toward distributed OPF using ALADIN. IEEE Transactions on Power Systems, 34(1), 584-594. [5] Engelmann, A., M\u00fchlpfordt, T., Jiang, Y., Houska, B., & Faulwasser, T. (2017). Distributed AC optimal power flow using ALADIN. IFAC-PapersOnLine, 50(1), 5536-5541. [6] Du, X., Engelmann, A., Jiang, Y., Faulwasser, T., & Houska, B. (2019). Distributed State Estimation for AC Power Systems using Gauss-Newton ALADIN. arXiv preprint arXiv:1903.08956.","title":"Application to power systems"},{"location":"furtherReading/#application-to-traffic-engineering","text":"","title":"Application to  Traffic engineering"},{"location":"furtherReading/#application-to-optimal-control","text":"","title":"Application to Optimal control"},{"location":"machLearn/","text":"An example from machine learning Here we give a simple classification example how ALADIN- \\alpha \\alpha can be used for machine learning problems. The goal here is finding a suitable parameter w w that classify the input data into its label. Loss Function Let x_j x_j be the j-th input data and y_j y_j be its label, let N N be the number of input data, let n_x n_x be the dimension of input data, let w w be the decision variable. We consider a \\ell_2 \\ell_2 -regularized logistic regression loss function \\begin{aligned} \\min_{w\\in \\mathbb{R}^d}\\; f(w) \\triangleq \\frac1N \\sum_{j=1}^N log(1+e^{-y_j x_j^T w})+\\frac{\\gamma}{2} \\lVert w \\rVert_2^2. \\end{aligned} \\begin{aligned} \\min_{w\\in \\mathbb{R}^d}\\; f(w) \\triangleq \\frac1N \\sum_{j=1}^N log(1+e^{-y_j x_j^T w})+\\frac{\\gamma}{2} \\lVert w \\rVert_2^2. \\end{aligned} We add the term \\frac{\\gamma}{2} \\lVert w \\rVert_2^2 \\frac{\\gamma}{2} \\lVert w \\rVert_2^2 in order to prevent overfitting, where \\gamma \\gamma is a hyperparameter chosen by user. Distributed Problem Setup To solve this problem, we divide the input data set into several groups and each group specifies a subsystem. We set the number of subsystems N_{subs} N_{subs} to 10, that is, the capacity of each subsystem is cap=N/N_{subs} cap=N/N_{subs} . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 %% set basic parameters import casadi.* ; data = importdata ( 'ML_DATA.mat' ); % import data set point = data . input ; % defeine data class = data . class ; % define label % connect point and their label set = [point,class] ; N = size(point, 1) ; % number of input data nx = size(point, 2) ; % dimension of input data Nsubs = 10 ; % number of subsystems cap = N/Nsubs ; gamma = 1 ; % choose hyperparameter gamma Next, we define the decision variable w w and set up the OCP problem. Because the volume of each group is cap cap and the dimension of data point is n_x n_x , the dimension of w w should be cap*n_x cap*n_x . It is obvious that we can also divide w w into cap cap groups, we denote it as w_j, j =1,2,...,cap w_j, j =1,2,...,cap . Therefore, equality constraints occurs naturally: \\begin{aligned} w_1 = w_2 = ... = w_{cap} \\end{aligned} \\begin{aligned} w_1 = w_2 = ... = w_{cap} \\end{aligned} Note that the form of the objective functions for each subsystem are same, so we define the objective function with parameter xy xy , which represents the input data and their labels for each subsystem. So we get: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 % define variables w = SX.sym('w', [cap*nx 1]) ; % decision variable xy = SX . sym ( 'xy' , [ cap * ( nx + 1 ) 1 ]); % data and label % set up objective function and constraints ff = 0 ; gg = []; for i = 1:cap j = (i-1)*nx ; k = (i-1)*(nx+1) ; ff = ff + 1/N*log(1+exp(xy(k+nx+1)*xy(k+1:nx)'*w(j+1:nx))) +... gamma/(2*N)*w(j+1:nx)'*w(j+1:nx) ; % objective function if i > 1 for p = 1:nx gg = [ gg ; w ( p ) - w ( j + p )]; % equality constraint end end end Next, we construct the consensus matrix \\{ A_i\\}, i=1,2,...N_{subs} \\{ A_i\\}, i=1,2,...N_{subs} . 1 2 3 4 5 6 7 8 % create elements used to set matrix A eyebase = eye ( nx * cap ); zerobase = zeros ( nx * cap ); % set up consensus constraints AA { 1 } = repmat ( eyebase , Nsubs - 1 , 1 ); for i = 2:Nsubs AA { i } = [ repmat ( zerobase , i - 2 , 1 ); eyebase ; repmat ( zerobase , Nsubs - i , 1 )]; end In the last step, we convert the CasADi symbolic expressions to the MATLAB functions and set up the initial guess z_i^0 z_i^0 and \\lambda_0 \\lambda_0 . Note that the objective function is parameterized with input data xy xy and the constraints for each subsystem are the same. 1 2 3 4 5 6 7 8 9 10 11 % convert expressions to MATLAB functions for i = 1 : Nsubs ML . locFuns . ffi { i } = Function ([ 'f' num2str ( i )], { w , xy }, { ff }); ML . locFuns . ggi { i } = Function ([ 'g' num2str ( i )], { w , xy }, { gg }); ML . locFuns . hhi { i } = Function ([ 'h' num2str ( i )], { w , xy }, {[]}); % lower and upper boundary of variables will be set as default values ML . AA { i } = AA { i }; ML . zz0 { i } = zeros ( nx * cap , 1 ); ML . p { i } = reshape ( set (( i - 1 ) * cap + 1 : cap , :) ' , [], 1 ); end Solution with ALADIN- \\alpha \\alpha To solve this distributed problem with ALDIN- \\alpha \\alpha , we still need to set up some options. 1 2 3 4 5 6 7 8 % initialize the options for ALADIN opts . rho = 1e3 ; opts . mu = 1e4 ; opts . maxiter = 10 ; opts . term_eps = 0 ; % no termination criterion, stop after maximum iteration opts . plot = 'true' ; sol_ML = run_ALADIN ( ML , opts ); If the option plot is true , we can see that the algorithm converges in about 5 iterations, which is quite fast.","title":"Machine Learing"},{"location":"machLearn/#an-example-from-machine-learning","text":"Here we give a simple classification example how ALADIN- \\alpha \\alpha can be used for machine learning problems. The goal here is finding a suitable parameter w w that classify the input data into its label.","title":"An example from machine learning"},{"location":"machLearn/#loss-function","text":"Let x_j x_j be the j-th input data and y_j y_j be its label, let N N be the number of input data, let n_x n_x be the dimension of input data, let w w be the decision variable. We consider a \\ell_2 \\ell_2 -regularized logistic regression loss function \\begin{aligned} \\min_{w\\in \\mathbb{R}^d}\\; f(w) \\triangleq \\frac1N \\sum_{j=1}^N log(1+e^{-y_j x_j^T w})+\\frac{\\gamma}{2} \\lVert w \\rVert_2^2. \\end{aligned} \\begin{aligned} \\min_{w\\in \\mathbb{R}^d}\\; f(w) \\triangleq \\frac1N \\sum_{j=1}^N log(1+e^{-y_j x_j^T w})+\\frac{\\gamma}{2} \\lVert w \\rVert_2^2. \\end{aligned} We add the term \\frac{\\gamma}{2} \\lVert w \\rVert_2^2 \\frac{\\gamma}{2} \\lVert w \\rVert_2^2 in order to prevent overfitting, where \\gamma \\gamma is a hyperparameter chosen by user.","title":"Loss Function"},{"location":"machLearn/#distributed-problem-setup","text":"To solve this problem, we divide the input data set into several groups and each group specifies a subsystem. We set the number of subsystems N_{subs} N_{subs} to 10, that is, the capacity of each subsystem is cap=N/N_{subs} cap=N/N_{subs} . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 %% set basic parameters import casadi.* ; data = importdata ( 'ML_DATA.mat' ); % import data set point = data . input ; % defeine data class = data . class ; % define label % connect point and their label set = [point,class] ; N = size(point, 1) ; % number of input data nx = size(point, 2) ; % dimension of input data Nsubs = 10 ; % number of subsystems cap = N/Nsubs ; gamma = 1 ; % choose hyperparameter gamma Next, we define the decision variable w w and set up the OCP problem. Because the volume of each group is cap cap and the dimension of data point is n_x n_x , the dimension of w w should be cap*n_x cap*n_x . It is obvious that we can also divide w w into cap cap groups, we denote it as w_j, j =1,2,...,cap w_j, j =1,2,...,cap . Therefore, equality constraints occurs naturally: \\begin{aligned} w_1 = w_2 = ... = w_{cap} \\end{aligned} \\begin{aligned} w_1 = w_2 = ... = w_{cap} \\end{aligned} Note that the form of the objective functions for each subsystem are same, so we define the objective function with parameter xy xy , which represents the input data and their labels for each subsystem. So we get: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 % define variables w = SX.sym('w', [cap*nx 1]) ; % decision variable xy = SX . sym ( 'xy' , [ cap * ( nx + 1 ) 1 ]); % data and label % set up objective function and constraints ff = 0 ; gg = []; for i = 1:cap j = (i-1)*nx ; k = (i-1)*(nx+1) ; ff = ff + 1/N*log(1+exp(xy(k+nx+1)*xy(k+1:nx)'*w(j+1:nx))) +... gamma/(2*N)*w(j+1:nx)'*w(j+1:nx) ; % objective function if i > 1 for p = 1:nx gg = [ gg ; w ( p ) - w ( j + p )]; % equality constraint end end end Next, we construct the consensus matrix \\{ A_i\\}, i=1,2,...N_{subs} \\{ A_i\\}, i=1,2,...N_{subs} . 1 2 3 4 5 6 7 8 % create elements used to set matrix A eyebase = eye ( nx * cap ); zerobase = zeros ( nx * cap ); % set up consensus constraints AA { 1 } = repmat ( eyebase , Nsubs - 1 , 1 ); for i = 2:Nsubs AA { i } = [ repmat ( zerobase , i - 2 , 1 ); eyebase ; repmat ( zerobase , Nsubs - i , 1 )]; end In the last step, we convert the CasADi symbolic expressions to the MATLAB functions and set up the initial guess z_i^0 z_i^0 and \\lambda_0 \\lambda_0 . Note that the objective function is parameterized with input data xy xy and the constraints for each subsystem are the same. 1 2 3 4 5 6 7 8 9 10 11 % convert expressions to MATLAB functions for i = 1 : Nsubs ML . locFuns . ffi { i } = Function ([ 'f' num2str ( i )], { w , xy }, { ff }); ML . locFuns . ggi { i } = Function ([ 'g' num2str ( i )], { w , xy }, { gg }); ML . locFuns . hhi { i } = Function ([ 'h' num2str ( i )], { w , xy }, {[]}); % lower and upper boundary of variables will be set as default values ML . AA { i } = AA { i }; ML . zz0 { i } = zeros ( nx * cap , 1 ); ML . p { i } = reshape ( set (( i - 1 ) * cap + 1 : cap , :) ' , [], 1 ); end","title":"Distributed Problem Setup"},{"location":"machLearn/#solution-with-aladin-alphaalpha","text":"To solve this distributed problem with ALDIN- \\alpha \\alpha , we still need to set up some options. 1 2 3 4 5 6 7 8 % initialize the options for ALADIN opts . rho = 1e3 ; opts . mu = 1e4 ; opts . maxiter = 10 ; opts . term_eps = 0 ; % no termination criterion, stop after maximum iteration opts . plot = 'true' ; sol_ML = run_ALADIN ( ML , opts ); If the option plot is true , we can see that the algorithm converges in about 5 iterations, which is quite fast.","title":"Solution with ALADIN-\\alpha\\alpha"},{"location":"options/","text":"Options ALADIN- \\alpha \\alpha comes with several ALADIN variants. These variants can be activated by setting options. All options with possible values can be found in loadDefOpts.m . Here, we give a little more detailled description of these options. Basic options ALADIN option default values alternative rho0 1e2 double > 0 rhoUpdate 1.1 double > 0 rhoMax 1e8 double > 0 Sig \u2018const\u2019 \u2018dyn\u2019 lamInit \u2018false\u2019 \u2018true\u2019 term_eps 0 double > 0 maxiter 30 integer > 0 mu0 1e3 double > 0 muUpdate 2 double > 0 muMax 2*1e6 double > 0 solveQP \u2018MA57\u2019 \u2018ipopt\u2019, \u2018pinv\u2019, \u2018linsolve\u2019, \u2018sparseBs\u2019, \u2018MOSEK\u2019, \u2018quadprog\u2019 loc Sol \u2018MA57\u2019 \u2018ipopt\u2019,\u2019sqpmethod\u2019 reg \u2018true\u2019 \u2018false\u2019 regParam 1e-4 double > 0 actMargin -1e-6 double < 0 plot \u2018true\u2019 \u2018false\u2019 Extensions ALADIN option default values alternative slack \u2018standard\u2019 \u2018redSpace\u2019 hessian \u2018standard\u2019 Hess \u2018standard\u2019 \u2018DBFGS\u2019, \\text{ } \\text{ } \u2018BFGS\u2019 BFGSinit \u2018ident\u2019 \u2018exact\u2019 parfor \u2018false\u2019 \u2018true\u2019 DelUp \u2018false\u2019 \u2018true\u2019 reuse \u2018false\u2019 \u2018true\u2019 commCount \u2018false\u2019 \u2018true\u2019 Bi-Level options ALADIN option default values alternative innnerAlg \u2018none\u2019 \u2018D-CG\u2019, \u2018D-ADMM\u2019 rhoADM 2e-2 warmStart \u2018true\u2019 \u2018false\u2019 innerIter 200 integer > 0 Parameter choices for initialization During the initialization process, the options for the parameters \\Sigma, \\rho 0, \\mu 0 \\Sigma, \\rho 0, \\mu 0 and \\varepsilon \\varepsilon are of relevance. The default settings are given by ALADIN option default values alternatives Sig \u2018const\u2019 \u2018dny\u2019 rho0 1e2 \\geq 0 \\geq 0 mu0 1e3 \\geq 0 \\geq 0 term_eps 0 \\geq 0 \\geq 0 The parameter \\texttt{rho0} \\texttt{rho0} represents the penalization of the distance (augmented step size) \\left|x_i-z_i^k\\right|_{\\Sigma_i}^2 \\left|x_i-z_i^k\\right|_{\\Sigma_i}^2 during the first parallel step. The option \\texttt{Sig} \\texttt{Sig} determines the augemented norm. In the first step, \\texttt{Sig} \\texttt{Sig} is the identity matrix, thus the norm |x_i - z_i| |x_i - z_i| is evaluated. Changing sigma leads to penalization of the distance |\\Sigma_i(x_i - z_i)| = |x_i - z_i|_{\\Sigma_i} |\\Sigma_i(x_i - z_i)| = |x_i - z_i|_{\\Sigma_i} . First, we recall, that the algorithm consists of several stepts, which are see here . Keeping the algorithm in mind, we can now focus on the options that can be selected. Basic Options Parameter \\rho \\rho During the Parallelizable Step k k , the optimization problems \\begin{aligned} \\min_{x_i\\in [\\underline {x_i}, \\overline x_i]} &f_i(x_i) + (\\lambda^k)^\\top A_i x_i + \\frac{\\rho^k}{2}\\left\\|x_i-z_i^k\\right\\|_{\\Sigma_i}^2 \\;\\; \\\\ \\text{s.t.}\\quad & g_i(x_i) = 0, \\; \\;h_i(x_i)\\leq 0,\\; \\;\\; \\underline{x_i} \\leq x_i \\leq \\overline{x}_i. \\end{aligned} \\begin{aligned} \\min_{x_i\\in [\\underline {x_i}, \\overline x_i]} &f_i(x_i) + (\\lambda^k)^\\top A_i x_i + \\frac{\\rho^k}{2}\\left\\|x_i-z_i^k\\right\\|_{\\Sigma_i}^2 \\;\\; \\\\ \\text{s.t.}\\quad & g_i(x_i) = 0, \\; \\;h_i(x_i)\\leq 0,\\; \\;\\; \\underline{x_i} \\leq x_i \\leq \\overline{x}_i. \\end{aligned} need to be solved for fixed z_i z_i . The parameter \\rho^k \\rho^k represents the penalization of the distance \\left|x_i-z_i^k\\right|_{\\Sigma_i}^2 \\left|x_i-z_i^k\\right|_{\\Sigma_i}^2 . As long as \\texttt{rho} \\texttt{rho} is smaller than \\texttt{rhoMax} \\texttt{rhoMax} , it is increased by factor \\texttt{rhoUpdate} \\texttt{rhoUpdate} . ALADIN option default values alternative rho0 1e2 double > 0 rhoUpdate 1.1 double > 0 rhoMax 1e8 double > 0 Dynamic \\Sigma \\Sigma The second parameter relevant for the Parallelizable Step is the scaling matrix \\Sigma \\Sigma . During the first iteration, \\Sigma \\Sigma equals the identity matrix. When the alternative option \u2018dyn\u2019 was selected, in each step the it is checked whether the step sizes for all variables decrease. In case that a step size is not decreasing in a sufficient manner, \\Sigma \\Sigma is changed dynamically to increase the negative impact of the large step size on the objective function in the parallel step. ALADIN option default value alternative Sig \u2018const\u2019 \u2018dyn\u2019 Parameter \\lambda \\lambda The parameter \\lambda \\lambda takes over the function of the lagrange multipliers. We can decide whether we want to hand over a specific one or not. ALADIN option default value alternative lamInit \u2018false\u2019 \u2018true\u2019 Termination criterion Executing the ALADIN algorithm, in each step a termination criterion is checked. The termination criterion is split into two parts. Part one checks, whether the maximum number \\texttt{maxiter} \\texttt{maxiter} of iterations is reached. Additionatlly, a termination bound \\varepsilon > 0 \\varepsilon > 0 can be handed over. Then, in each step, it is terminated, if \\left\\|\\sum_{i\\in \\mathcal{R}}A_ix^k_i -b \\right\\|\\leq \\epsilon \\text{ and } \\left\\| x^k - z^k \\right \\|\\leq \\epsilon\\;, \\left\\|\\sum_{i\\in \\mathcal{R}}A_ix^k_i -b \\right\\|\\leq \\epsilon \\text{ and } \\left\\| x^k - z^k \\right \\|\\leq \\epsilon\\;, holds true. ALADIN option default value alternative term_eps 0 double > 0 maxiter 30 integer > 0 Parameter \\mu \\mu During the consensus step, the coordination QP \\begin{aligned} &\\underset{\\Delta x,s}{\\min}\\;\\;\\sum_{i\\in \\mathcal{R}}\\left\\{\\frac{1}{2}\\Delta x_i^\\top B^k_i\\Delta x_i + {g_i^k}^\\top \\Delta x_i\\right\\} + (\\lambda^k)^\\top s + \\frac{\\mu^k}{2}\\|s\\|^2_2 \\\\ & \\begin{aligned} \\text{subject to}\\; \\sum_{i\\in \\mathcal{R}}A_i(x^k_i+\\Delta x_i) &= s \\qquad |\\; \\lambda^{\\mathrm{QP} k},\\\\ C^k_i \\Delta x_i &= 0 \\qquad \\forall i\\in \\mathcal{R},\\\\ \\end{aligned} \\end{aligned} \\begin{aligned} &\\underset{\\Delta x,s}{\\min}\\;\\;\\sum_{i\\in \\mathcal{R}}\\left\\{\\frac{1}{2}\\Delta x_i^\\top B^k_i\\Delta x_i + {g_i^k}^\\top \\Delta x_i\\right\\} + (\\lambda^k)^\\top s + \\frac{\\mu^k}{2}\\|s\\|^2_2 \\\\ & \\begin{aligned} \\text{subject to}\\; \\sum_{i\\in \\mathcal{R}}A_i(x^k_i+\\Delta x_i) &= s \\qquad |\\; \\lambda^{\\mathrm{QP} k},\\\\ C^k_i \\Delta x_i &= 0 \\qquad \\forall i\\in \\mathcal{R},\\\\ \\end{aligned} \\end{aligned} has to be executed. Setting s:= \\sum A_ix_i - b \\overset{!}{=} 0 s:= \\sum A_ix_i - b \\overset{!}{=} 0 , the parameter \\mu \\mu is similarly to the parameter \\rho \\rho from above a penalty parameter. It can be set in the same manner as \\rho \\rho . ALADIN option default values alternative mu0 1e3 double > 0 muUpdate 2 double > 0 muMax 2*1e6 double > 0 Solvers To solve the two optimization problems in each iteration step, the desired solvers can be indicated via setting the option opts.solveQP and opts.locSol: 1. QP Solver ALADIN option default value alternatives solveQP \u2018MA57\u2019 \u2018ipopt\u2019, \u2018pinv\u2019, \u2018linsolve\u2019, \u2018sparseBs\u2019, \u2018MOSEK\u2019, \u2018quadprog\u2019 2. Local Solver ALADIN option default value alternatives locSol \u2018ipopt\u2019 \u2018ipopt\u2019, \u2018sqpmethod\u2019 Regularization Parameters Sometimes the Hessian matrices of the given problems do not have full rank, such that some important matrix operations are not readily available. Remedy can be obtained by regularization approaches that increase the rank of the matrix. The option can be set as follows: ALADIN option default value alternative reg \u2018true\u2019 \u2018false\u2019 regParam 1e-4 Active Margin Detection ALADIN, beeing a solver for constraint optimization, needs an active margin detection. The tolerance is handed over by the option \\texttt{actMargin} \\texttt{actMargin} ALADIN option default value actMargin -1e-6 Plots Plotting results is nice, because one can immediately see the results. However, push up windows spreading plots over your screen can be annoying and slow down the algorithm, so we included an option to deactivate plots :) ALADIN option default value alternative plot \u2018true\u2019 \u2018false\u2019 Extensions parfor Option The parallelizable step from the ALADIN Algorithm can be executed in the parallel threads in matlab using its implemented parfor -loop. This can be activeted by: ALADIN option default value alternative parfor \u2018false\u2019 \u2018true\u2019 Bilevel Options Parameter combinations Note that ss ALADIN- \\alpha \\alpha is still in a prototypical phase of development, it is not guaranteed that all combinations of options work. We tried to make ALADIN- \\alpha \\alpha as stable as possible running tests with a high code coverage, but we are at the moment not able to guarantee that all combinations of options work","title":"Options"},{"location":"options/#options","text":"ALADIN- \\alpha \\alpha comes with several ALADIN variants. These variants can be activated by setting options. All options with possible values can be found in loadDefOpts.m . Here, we give a little more detailled description of these options. Basic options ALADIN option default values alternative rho0 1e2 double > 0 rhoUpdate 1.1 double > 0 rhoMax 1e8 double > 0 Sig \u2018const\u2019 \u2018dyn\u2019 lamInit \u2018false\u2019 \u2018true\u2019 term_eps 0 double > 0 maxiter 30 integer > 0 mu0 1e3 double > 0 muUpdate 2 double > 0 muMax 2*1e6 double > 0 solveQP \u2018MA57\u2019 \u2018ipopt\u2019, \u2018pinv\u2019, \u2018linsolve\u2019, \u2018sparseBs\u2019, \u2018MOSEK\u2019, \u2018quadprog\u2019 loc Sol \u2018MA57\u2019 \u2018ipopt\u2019,\u2019sqpmethod\u2019 reg \u2018true\u2019 \u2018false\u2019 regParam 1e-4 double > 0 actMargin -1e-6 double < 0 plot \u2018true\u2019 \u2018false\u2019 Extensions ALADIN option default values alternative slack \u2018standard\u2019 \u2018redSpace\u2019 hessian \u2018standard\u2019 Hess \u2018standard\u2019 \u2018DBFGS\u2019, \\text{ } \\text{ } \u2018BFGS\u2019 BFGSinit \u2018ident\u2019 \u2018exact\u2019 parfor \u2018false\u2019 \u2018true\u2019 DelUp \u2018false\u2019 \u2018true\u2019 reuse \u2018false\u2019 \u2018true\u2019 commCount \u2018false\u2019 \u2018true\u2019 Bi-Level options ALADIN option default values alternative innnerAlg \u2018none\u2019 \u2018D-CG\u2019, \u2018D-ADMM\u2019 rhoADM 2e-2 warmStart \u2018true\u2019 \u2018false\u2019 innerIter 200 integer > 0 Parameter choices for initialization During the initialization process, the options for the parameters \\Sigma, \\rho 0, \\mu 0 \\Sigma, \\rho 0, \\mu 0 and \\varepsilon \\varepsilon are of relevance. The default settings are given by ALADIN option default values alternatives Sig \u2018const\u2019 \u2018dny\u2019 rho0 1e2 \\geq 0 \\geq 0 mu0 1e3 \\geq 0 \\geq 0 term_eps 0 \\geq 0 \\geq 0 The parameter \\texttt{rho0} \\texttt{rho0} represents the penalization of the distance (augmented step size) \\left|x_i-z_i^k\\right|_{\\Sigma_i}^2 \\left|x_i-z_i^k\\right|_{\\Sigma_i}^2 during the first parallel step. The option \\texttt{Sig} \\texttt{Sig} determines the augemented norm. In the first step, \\texttt{Sig} \\texttt{Sig} is the identity matrix, thus the norm |x_i - z_i| |x_i - z_i| is evaluated. Changing sigma leads to penalization of the distance |\\Sigma_i(x_i - z_i)| = |x_i - z_i|_{\\Sigma_i} |\\Sigma_i(x_i - z_i)| = |x_i - z_i|_{\\Sigma_i} . First, we recall, that the algorithm consists of several stepts, which are see here . Keeping the algorithm in mind, we can now focus on the options that can be selected.","title":"Options"},{"location":"options/#basic-options","text":"Parameter \\rho \\rho During the Parallelizable Step k k , the optimization problems \\begin{aligned} \\min_{x_i\\in [\\underline {x_i}, \\overline x_i]} &f_i(x_i) + (\\lambda^k)^\\top A_i x_i + \\frac{\\rho^k}{2}\\left\\|x_i-z_i^k\\right\\|_{\\Sigma_i}^2 \\;\\; \\\\ \\text{s.t.}\\quad & g_i(x_i) = 0, \\; \\;h_i(x_i)\\leq 0,\\; \\;\\; \\underline{x_i} \\leq x_i \\leq \\overline{x}_i. \\end{aligned} \\begin{aligned} \\min_{x_i\\in [\\underline {x_i}, \\overline x_i]} &f_i(x_i) + (\\lambda^k)^\\top A_i x_i + \\frac{\\rho^k}{2}\\left\\|x_i-z_i^k\\right\\|_{\\Sigma_i}^2 \\;\\; \\\\ \\text{s.t.}\\quad & g_i(x_i) = 0, \\; \\;h_i(x_i)\\leq 0,\\; \\;\\; \\underline{x_i} \\leq x_i \\leq \\overline{x}_i. \\end{aligned} need to be solved for fixed z_i z_i . The parameter \\rho^k \\rho^k represents the penalization of the distance \\left|x_i-z_i^k\\right|_{\\Sigma_i}^2 \\left|x_i-z_i^k\\right|_{\\Sigma_i}^2 . As long as \\texttt{rho} \\texttt{rho} is smaller than \\texttt{rhoMax} \\texttt{rhoMax} , it is increased by factor \\texttt{rhoUpdate} \\texttt{rhoUpdate} . ALADIN option default values alternative rho0 1e2 double > 0 rhoUpdate 1.1 double > 0 rhoMax 1e8 double > 0 Dynamic \\Sigma \\Sigma The second parameter relevant for the Parallelizable Step is the scaling matrix \\Sigma \\Sigma . During the first iteration, \\Sigma \\Sigma equals the identity matrix. When the alternative option \u2018dyn\u2019 was selected, in each step the it is checked whether the step sizes for all variables decrease. In case that a step size is not decreasing in a sufficient manner, \\Sigma \\Sigma is changed dynamically to increase the negative impact of the large step size on the objective function in the parallel step. ALADIN option default value alternative Sig \u2018const\u2019 \u2018dyn\u2019 Parameter \\lambda \\lambda The parameter \\lambda \\lambda takes over the function of the lagrange multipliers. We can decide whether we want to hand over a specific one or not. ALADIN option default value alternative lamInit \u2018false\u2019 \u2018true\u2019 Termination criterion Executing the ALADIN algorithm, in each step a termination criterion is checked. The termination criterion is split into two parts. Part one checks, whether the maximum number \\texttt{maxiter} \\texttt{maxiter} of iterations is reached. Additionatlly, a termination bound \\varepsilon > 0 \\varepsilon > 0 can be handed over. Then, in each step, it is terminated, if \\left\\|\\sum_{i\\in \\mathcal{R}}A_ix^k_i -b \\right\\|\\leq \\epsilon \\text{ and } \\left\\| x^k - z^k \\right \\|\\leq \\epsilon\\;, \\left\\|\\sum_{i\\in \\mathcal{R}}A_ix^k_i -b \\right\\|\\leq \\epsilon \\text{ and } \\left\\| x^k - z^k \\right \\|\\leq \\epsilon\\;, holds true. ALADIN option default value alternative term_eps 0 double > 0 maxiter 30 integer > 0 Parameter \\mu \\mu During the consensus step, the coordination QP \\begin{aligned} &\\underset{\\Delta x,s}{\\min}\\;\\;\\sum_{i\\in \\mathcal{R}}\\left\\{\\frac{1}{2}\\Delta x_i^\\top B^k_i\\Delta x_i + {g_i^k}^\\top \\Delta x_i\\right\\} + (\\lambda^k)^\\top s + \\frac{\\mu^k}{2}\\|s\\|^2_2 \\\\ & \\begin{aligned} \\text{subject to}\\; \\sum_{i\\in \\mathcal{R}}A_i(x^k_i+\\Delta x_i) &= s \\qquad |\\; \\lambda^{\\mathrm{QP} k},\\\\ C^k_i \\Delta x_i &= 0 \\qquad \\forall i\\in \\mathcal{R},\\\\ \\end{aligned} \\end{aligned} \\begin{aligned} &\\underset{\\Delta x,s}{\\min}\\;\\;\\sum_{i\\in \\mathcal{R}}\\left\\{\\frac{1}{2}\\Delta x_i^\\top B^k_i\\Delta x_i + {g_i^k}^\\top \\Delta x_i\\right\\} + (\\lambda^k)^\\top s + \\frac{\\mu^k}{2}\\|s\\|^2_2 \\\\ & \\begin{aligned} \\text{subject to}\\; \\sum_{i\\in \\mathcal{R}}A_i(x^k_i+\\Delta x_i) &= s \\qquad |\\; \\lambda^{\\mathrm{QP} k},\\\\ C^k_i \\Delta x_i &= 0 \\qquad \\forall i\\in \\mathcal{R},\\\\ \\end{aligned} \\end{aligned} has to be executed. Setting s:= \\sum A_ix_i - b \\overset{!}{=} 0 s:= \\sum A_ix_i - b \\overset{!}{=} 0 , the parameter \\mu \\mu is similarly to the parameter \\rho \\rho from above a penalty parameter. It can be set in the same manner as \\rho \\rho . ALADIN option default values alternative mu0 1e3 double > 0 muUpdate 2 double > 0 muMax 2*1e6 double > 0 Solvers To solve the two optimization problems in each iteration step, the desired solvers can be indicated via setting the option opts.solveQP and opts.locSol: 1. QP Solver ALADIN option default value alternatives solveQP \u2018MA57\u2019 \u2018ipopt\u2019, \u2018pinv\u2019, \u2018linsolve\u2019, \u2018sparseBs\u2019, \u2018MOSEK\u2019, \u2018quadprog\u2019 2. Local Solver ALADIN option default value alternatives locSol \u2018ipopt\u2019 \u2018ipopt\u2019, \u2018sqpmethod\u2019 Regularization Parameters Sometimes the Hessian matrices of the given problems do not have full rank, such that some important matrix operations are not readily available. Remedy can be obtained by regularization approaches that increase the rank of the matrix. The option can be set as follows: ALADIN option default value alternative reg \u2018true\u2019 \u2018false\u2019 regParam 1e-4 Active Margin Detection ALADIN, beeing a solver for constraint optimization, needs an active margin detection. The tolerance is handed over by the option \\texttt{actMargin} \\texttt{actMargin} ALADIN option default value actMargin -1e-6 Plots Plotting results is nice, because one can immediately see the results. However, push up windows spreading plots over your screen can be annoying and slow down the algorithm, so we included an option to deactivate plots :) ALADIN option default value alternative plot \u2018true\u2019 \u2018false\u2019","title":"Basic Options"},{"location":"options/#extensions","text":"parfor Option The parallelizable step from the ALADIN Algorithm can be executed in the parallel threads in matlab using its implemented parfor -loop. This can be activeted by: ALADIN option default value alternative parfor \u2018false\u2019 \u2018true\u2019","title":"Extensions"},{"location":"options/#bilevel-options","text":"Parameter combinations Note that ss ALADIN- \\alpha \\alpha is still in a prototypical phase of development, it is not guaranteed that all combinations of options work. We tried to make ALADIN- \\alpha \\alpha as stable as possible running tests with a high code coverage, but we are at the moment not able to guarantee that all combinations of options work","title":"Bilevel Options"},{"location":"redComm/","text":"Reducing Communication There are different ways of reducing the communication overhead in ALADIN- \\alpha \\alpha . Compared with ADMM for example, the coordination step of ALADIN is quite heavy due to the necessity to exchange derivative information. However, there are different ways to reduce the communication overhead of ALADIN- \\alpha \\alpha . Which one works best also depends on the problem at hand. The first option is to use Hessian approximations instead of exact Hessians. The advantage here is, that with the Broyden-Fletcher\u2013Goldfarb-Shanno (BFGS) formula for example, the Hessian matrix is approximated based on two subsequent gradients. Gradients scale with O(n) O(n) whereas symmetric matrices with O(n^2) O(n^2) in general making Hessian approximations preferable over exact Hessians. However, one has to keep in mind that due to their approximative nature, ALADIM- \\alpha \\alpha might need more iterations to convergence which might compensate the saved communication overhead per iteration. BFGS and damped BFGS can be activated in ALADIN- \\alpha \\alpha with the Hess option with BFGS or DBFGS respectively. Another option is based on the so-called reduced-space or nullspace method (cf. Chapter 18 in [1] for example). The idea here is to eliminate all active constraints by computing a basis of the nullspace they span. With that, the dimension of the problem can be reduced by two-times the number of constraints and thus the communication-overhead can be substantially smaller - especially in case of many active constraints. A further improvement of the nullspace-method is using the Schur-complement . Here, the dimension of the coordination problem in ALADIN can be reduced to the number of coupling variables which is in general much smaller than the original coordination QP. On this reduced problem it is then also possible to use the both proposed variants of bi-level ALADIN which solves this reduced coordination problem purely on a neighbor-to-neighbor baisis yielding one of the first decentralized algorithms for non-convex optimization with convergence guarantees. The used inner algorithms are variants of the Alternating Direction of Multipliers Method (ADMM) and the Conjugate Gradient (CG) method. For details on all of the above mentioned methods we refer to [1] and [3]. Numerical Example: Optimal Power Flow Next, we give a numerical example showing the benefits of the above mentioned methods. We load a given problem formulation containing an Optimal Power FLow problem with 30 buses from [2]. The data is included in ALADIN- \\alpha \\alpha in the ./test/problem_data/ folder. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 load ( './test/problem_data/IEEE30busPrbFrm.mat' ) opts . commCount = 'true' ; %% standard ALADIN opts . maxiter = 30 ; opts . innerAlg = 'none' ; opts . slack = 'standard' ; opts . Hess = 'standard' ; res_ALADINs = run_ALADINnew(sProb, opts) ; %% damped BFGS opts . Hess = 'DBFGS' ; % damped BFGS opts . slack = 'standard' ; opts . BFGSinit = 'exact' ; % with exact Hessian initialization res_ALADINbf = run_ALADINnew(sProb, opts) ; %% reduced-space method opts . slack = 'redSpace' ; opts . Hess = 'standard' ; res_ALADINred = run_ALADINnew(sProb, opts) ; %% bi-level ALADIN with D-CG opts . innerAlg = 'D-CG' ; opts . innerIter = 50 ; opts . Hess = 'standard' ; opts . slack = 'standard' ; res_ALADINcg = run_ALADINnew(sProb, opts) ; %% bi-level ALADIN with D-ADMM opts . innerAlg = 'D-ADMM' ; opts . innerIter = 130 ; opts . Hess = 'standard' ; opts . slack = 'standard' ; res_ALADINadm = run_ALADINnew(sProb, opts) ; By activating the commCount option, global and neighbor-neighbor communication is counted during the ALADIN- \\alpha \\alpha iterations. An exemplary solver output looks for standard ALADIN like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ======================================================== == This is ALADIN - $\\ alpha $ v0 .1 == ======================================================== QP solver : MA57 Local solver : ipopt Inner algorithm : none No termination criterion was specified . Consensus violation : 3.3407e-12 Maximum number of iterations reached . ----------------- ALADIN-$\\alpha$ timing ------------------ t [ s ] % tot % iter Tot time : ...... : 7.7 Prob setup :.... : 1.5 19.8 Iter time : ..... : 6.2 80.0 ------ NLP time : ...... : 2.6 42.3 QP time : ....... : 0.2 3.8 Reg time : ...... : 0.1 1.2 Plot time : ..... : 2.9 46.9 -------------- Communication analysis ---------------- floats tot avg Global forward :.... : 344176 11473 Neighbor - neighbor :. : 0 0 ======================================================== Note that we have an output where the communication overhead is counted. The results for the average communication per ALADIN- \\alpha \\alpha iteration are shown below for all the above cases in floats not exploiting sparsity in the Hessian of the Lagrangian and the Jacobian of the constraints. Note that we only count forward communication here. This means that we count the communication from the subsystems to the coordinator as this type of communication dominates the backward communication. ALADIN variant standard D-BFGS red.-space bi-level CG bi-level ADMM global 11.473 7.245 1663 8 - neighbor-neighbor - - - 64 64 From this table, one can clearly see that using a reduced-space method can reduce the per-step communication overhead substantially for this example. Even more, we can use bi-level ALADIN reducing the communication overhead even further. However, consider that due to the different inaccuracies introduce in bi-level ALADIN for example, one might need more iterations in total as shown in [2]. References [1] Nocedal, J., & Wright, S. (2006). Numerical optimization. Springer Science & Business Media. [2] Engelmann, A., Jiang, Y., Houska, B., & Faulwasser, T. (2019). Decomposition of non-convex optimization via bi-level distributed ALADIN. arXiv preprint arXiv:1903.11280. [3] Engelmann, A., Jiang, Y., Houska, B., & Faulwasser, T. (2019). ALADIN- \\alpha \\alpha \u2013 An open-source MATLAB toolbox for distributed optimization.","title":"Reducing Communication"},{"location":"redComm/#reducing-communication","text":"There are different ways of reducing the communication overhead in ALADIN- \\alpha \\alpha . Compared with ADMM for example, the coordination step of ALADIN is quite heavy due to the necessity to exchange derivative information. However, there are different ways to reduce the communication overhead of ALADIN- \\alpha \\alpha . Which one works best also depends on the problem at hand. The first option is to use Hessian approximations instead of exact Hessians. The advantage here is, that with the Broyden-Fletcher\u2013Goldfarb-Shanno (BFGS) formula for example, the Hessian matrix is approximated based on two subsequent gradients. Gradients scale with O(n) O(n) whereas symmetric matrices with O(n^2) O(n^2) in general making Hessian approximations preferable over exact Hessians. However, one has to keep in mind that due to their approximative nature, ALADIM- \\alpha \\alpha might need more iterations to convergence which might compensate the saved communication overhead per iteration. BFGS and damped BFGS can be activated in ALADIN- \\alpha \\alpha with the Hess option with BFGS or DBFGS respectively. Another option is based on the so-called reduced-space or nullspace method (cf. Chapter 18 in [1] for example). The idea here is to eliminate all active constraints by computing a basis of the nullspace they span. With that, the dimension of the problem can be reduced by two-times the number of constraints and thus the communication-overhead can be substantially smaller - especially in case of many active constraints. A further improvement of the nullspace-method is using the Schur-complement . Here, the dimension of the coordination problem in ALADIN can be reduced to the number of coupling variables which is in general much smaller than the original coordination QP. On this reduced problem it is then also possible to use the both proposed variants of bi-level ALADIN which solves this reduced coordination problem purely on a neighbor-to-neighbor baisis yielding one of the first decentralized algorithms for non-convex optimization with convergence guarantees. The used inner algorithms are variants of the Alternating Direction of Multipliers Method (ADMM) and the Conjugate Gradient (CG) method. For details on all of the above mentioned methods we refer to [1] and [3].","title":"Reducing Communication"},{"location":"redComm/#numerical-example-optimal-power-flow","text":"Next, we give a numerical example showing the benefits of the above mentioned methods. We load a given problem formulation containing an Optimal Power FLow problem with 30 buses from [2]. The data is included in ALADIN- \\alpha \\alpha in the ./test/problem_data/ folder. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 load ( './test/problem_data/IEEE30busPrbFrm.mat' ) opts . commCount = 'true' ; %% standard ALADIN opts . maxiter = 30 ; opts . innerAlg = 'none' ; opts . slack = 'standard' ; opts . Hess = 'standard' ; res_ALADINs = run_ALADINnew(sProb, opts) ; %% damped BFGS opts . Hess = 'DBFGS' ; % damped BFGS opts . slack = 'standard' ; opts . BFGSinit = 'exact' ; % with exact Hessian initialization res_ALADINbf = run_ALADINnew(sProb, opts) ; %% reduced-space method opts . slack = 'redSpace' ; opts . Hess = 'standard' ; res_ALADINred = run_ALADINnew(sProb, opts) ; %% bi-level ALADIN with D-CG opts . innerAlg = 'D-CG' ; opts . innerIter = 50 ; opts . Hess = 'standard' ; opts . slack = 'standard' ; res_ALADINcg = run_ALADINnew(sProb, opts) ; %% bi-level ALADIN with D-ADMM opts . innerAlg = 'D-ADMM' ; opts . innerIter = 130 ; opts . Hess = 'standard' ; opts . slack = 'standard' ; res_ALADINadm = run_ALADINnew(sProb, opts) ; By activating the commCount option, global and neighbor-neighbor communication is counted during the ALADIN- \\alpha \\alpha iterations. An exemplary solver output looks for standard ALADIN like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ======================================================== == This is ALADIN - $\\ alpha $ v0 .1 == ======================================================== QP solver : MA57 Local solver : ipopt Inner algorithm : none No termination criterion was specified . Consensus violation : 3.3407e-12 Maximum number of iterations reached . ----------------- ALADIN-$\\alpha$ timing ------------------ t [ s ] % tot % iter Tot time : ...... : 7.7 Prob setup :.... : 1.5 19.8 Iter time : ..... : 6.2 80.0 ------ NLP time : ...... : 2.6 42.3 QP time : ....... : 0.2 3.8 Reg time : ...... : 0.1 1.2 Plot time : ..... : 2.9 46.9 -------------- Communication analysis ---------------- floats tot avg Global forward :.... : 344176 11473 Neighbor - neighbor :. : 0 0 ======================================================== Note that we have an output where the communication overhead is counted. The results for the average communication per ALADIN- \\alpha \\alpha iteration are shown below for all the above cases in floats not exploiting sparsity in the Hessian of the Lagrangian and the Jacobian of the constraints. Note that we only count forward communication here. This means that we count the communication from the subsystems to the coordinator as this type of communication dominates the backward communication. ALADIN variant standard D-BFGS red.-space bi-level CG bi-level ADMM global 11.473 7.245 1663 8 - neighbor-neighbor - - - 64 64 From this table, one can clearly see that using a reduced-space method can reduce the per-step communication overhead substantially for this example. Even more, we can use bi-level ALADIN reducing the communication overhead even further. However, consider that due to the different inaccuracies introduce in bi-level ALADIN for example, one might need more iterations in total as shown in [2].","title":"Numerical Example: Optimal Power Flow"},{"location":"redComm/#references","text":"[1] Nocedal, J., & Wright, S. (2006). Numerical optimization. Springer Science & Business Media. [2] Engelmann, A., Jiang, Y., Houska, B., & Faulwasser, T. (2019). Decomposition of non-convex optimization via bi-level distributed ALADIN. arXiv preprint arXiv:1903.11280. [3] Engelmann, A., Jiang, Y., Houska, B., & Faulwasser, T. (2019). ALADIN- \\alpha \\alpha \u2013 An open-source MATLAB toolbox for distributed optimization.","title":"References"},{"location":"robotEx/","text":"Distributed MPC for Mobile Robots Here we give an example how ALADIN-M can be used for distributed Model Predictive Control. In particular, we show how the distributed parametric programming option and the the problem reuse option of ALADIN-M are useful. The example is similar to the example from [1]. The goal here is that two mobile robots exchange their positions while keeping a certain distance. This task can be formulated as a continuous-time optimal control problem (OCP) \\begin{aligned} &\\min_{x_i(),u_i(), \\forall i \\in \\mathcal{R}} \\int_0^T \\sum_{i\\in \\mathcal{R}} \\|x_i-x^e_i\\|_{Q_i}^2 + \\|u_i\\|_{R_i}^2\\, dt \\\\ \\quad \\text{s.t.} \\quad & \\dot x_i(t) = f_i(x_i(t),u_i(t)), x_i(0)=z_{i0}, && \\forall i \\in \\mathcal{R} \\\\ &(z,y)_i^\\top(T)=(z^e,y^e)_i^\\top, &&\\forall i \\in \\mathcal{R} \\\\ & \\|(z, y)_i^\\top(t)-(z, y)_j^\\top(t)\\|_2^2\\geq d^2, && i \\neq j. \\end{aligned} \\begin{aligned} &\\min_{x_i(),u_i(), \\forall i \\in \\mathcal{R}} \\int_0^T \\sum_{i\\in \\mathcal{R}} \\|x_i-x^e_i\\|_{Q_i}^2 + \\|u_i\\|_{R_i}^2\\, dt \\\\ \\quad \\text{s.t.} \\quad & \\dot x_i(t) = f_i(x_i(t),u_i(t)), x_i(0)=z_{i0}, && \\forall i \\in \\mathcal{R} \\\\ &(z,y)_i^\\top(T)=(z^e,y^e)_i^\\top, &&\\forall i \\in \\mathcal{R} \\\\ & \\|(z, y)_i^\\top(t)-(z, y)_j^\\top(t)\\|_2^2\\geq d^2, && i \\neq j. \\end{aligned} Here, z_i=(x_i\\; y_i\\; \\theta_i)^\\top z_i=(x_i\\; y_i\\; \\theta_i)^\\top is the state of each robot i \\in \\mathcal{R} i \\in \\mathcal{R} , x_i x_i and y_i y_i describe the robots position in the x x - y y -plane, and \\theta_i \\theta_i is the yaw angle with respect to the x x -axis (cf. figure below). Note that the initial condition can here be interpreted as the parameter p_i p_i in the format suitable for ALADIN- \\alpha \\alpha . The robots\u2019 dynamics is given by $$ \\dot x_i= f_i(x_i,u_i) := \\begin{pmatrix} v_i\\cos (\\theta_i) & v_i\\sin(\\theta_i) & \\omega_i \\end{pmatrix}^\\top , \\; %\\begin{pmatrix} %\\cos (\\theta_i) & 0 \\ %\\sin(\\theta_i) & 0 \\ %0 & 1 %\\end{pmatrix} %\\begin{pmatrix} %v_i \\ \\omega_i %\\end{pmatrix}, \\quad i \\in {1,2}. $$ Distributed Problem Setup Code-wise, we set up the right-hand-side of the ode dynamics from above as 1 2 3 4 % define robot models ode = @( x , u ) [ u ( 1 ) * cos ( x ( 3 )); u ( 1 ) * sin ( x ( 3 )); u ( 2 )]; Next, we prepare ourself for setting ob the robots\u2019 OCPs. We construct our problem using CasADi. Specifically, we create cells XX and UU containing the CasADi symbolic variables for the states z z and the inputs u u over the horizon-length T T . Moreover, we introduce a cell with state copies ZZZ containing the state information of neighboring robots in order to allow each robot to fulfill the distance inequality constraint {\\|(z, y)_i^\\top(t)-(z, y)_j^\\top(t)\\|_2^2}\\geq d^2 {\\|(z, y)_i^\\top(t)-(z, y)_j^\\top(t)\\|_2^2}\\geq d^2 . Note that we will enforce the copied and the original state to coincide later by the consensus constraint \\sum_{i\\in \\mathcal{R}} A_ix_i=b \\sum_{i\\in \\mathcal{R}} A_ix_i=b in the ALADIN- \\alpha \\alpha format . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 %% set up OCP import casadi.* Nrobot = 2 ; T = 1 ; % Time horizon dT = 0.1 ; % sampling time d = 2 ; % minimal distance between robots N = T/dT ; Nmpc = 40 ; for i=1:Nrobot % inputs/states XX { i } = SX . sym ([ 'x' num2str ( i )], [ 3 N ]); % state copies for k=setdiff(1:Nrobot,i) ZZZ { i }{ k } = SX . sym ([ 'z' num2str ( i ) num2str ( k )], [ 3 N ]); end UU{i} = SX.sym(['u' num2str(i)], [2 N]) ; XX0 { i } = SX . sym ([ 'xx0' num2str ( i )], [ 3 1 ]); end Now we are ready to set-up a discretized version of the above OCP. Note that we construct this OCP in a loop setting up the OCPs for all robots individually. Here we use a Heun-discretization scheme. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 % starting points/destinations % from to ppNum { 1 } = [ 0 10 ; 0 10 0 0 ] ; ppNum { 2 } = [ 10 0 ; 10 0 ; 0 0 ] ; % for each robot ... for i = 1 : Nrobot JJ { i } = 0 ; gg { i } = []; hh { i } = []; llbu { i } = []; uubu { i } = []; % over horizon ... for j=1:N - 1 % ode/stage cost with Heun discretization gg { i } = [ gg { i }; XX { i }(:, j + 1 ) - XX { i }(:, j ) - dT * 0.5 * ( ode ( XX { i }(:, j ), UU { i }(:, j )) + ode ( XX { i }(:, j + 1 ), UU { i }(:, j + 1 )))]; JJ { i } = JJ { i } + ( XX { i }(:, j ) - ppNum { i }(:, 2 )) '* diag ([ 1 1 0 ]) * ( XX { i }(:, j ) - ppNum { i }(:, 2 )) ... + UU { i }(:, j ) '* UU { i }(:, j ); % distance constraint hh { i } = [ hh { i }; - ( XX { i }( 1 : 2 , j ) - ZZZ { i }{ 3 - i }( 1 : 2 , j )) '* ( XX { i }( 1 : 2 , j ) - ZZZ { i }{ 3 - i }( 1 : 2 , j )) + d ^ 2 ]; end % initial condition gg { i } = [ gg { i }; XX { i }(:, 1 ) - XX0 { i }]; end for i=1:Nrobot ZZZ { i }{ i } = XX { i }; ZZZi = vertcat(vertcat(ZZZ{i}{:})) ; UUi = UU{i} ; XXU { i } = [ ZZZi (:); UUi (:)]; end Next, we construct the consensus matrices \\{A_i\\}_{i \\in \\mathcal{R}} \\{A_i\\}_{i \\in \\mathcal{R}} . As mentioned before, we construct them such that the original trajectories coincide with the copied trajectories. 1 2 3 4 5 6 7 % set up consensus constraints Abase = [ eye(Nrobot*N*3) zeros(Nrobot*N*3,2*N)] ; zerBase = zeros ( size ( Abase )); for i=1:Nrobot-1 AA { i } = [ repmat ( zerBase , i - 1 , 1 ); Abase ; repmat ( zerBase , Nrobot - i - 1 , 1 )]; end AA{Nrobot} = - repmat(Abase,Nrobot-1,1) ; In a last step, we convert the CasADi symbolic expressions to evaluable and set up the initial guesses z_i^0 z_i^0 and \\lambda^0 \\lambda^0 . Note that the local equality constraints collected in rob.locFuns.gg are parametrized with the initial condition X0 for the ode of the robots here. With that, we will be able to efficiently reuse the problem formulation in an MPC loop as we shall see next. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 % convert expressions to MATLAB functions X0 = vertcat(XX0{:}) ; ppNumAll = [ ppNum { 1 }(:, 1 ); ppNum { 2 }(:, 1 )]; for i=1:Nrobot rob . locFuns . ffi { i } = Function ([ 'f' num2str ( i )],{ XXU { i }},{ JJ { i }}); rob . locFuns . ggi { i } = Function ([ 'g' num2str ( i )],{[ XXU { i }; X0 ]},{ gg { i }}); rob . locFuns . hhi { i } = Function ([ 'h' num2str ( i )],{ XXU { i }},{ hh { i }}); % set up ALADIN parameters rob . llbx { i } = - inf * ones ( length ( XXU { i }), 1 ); rob . uubx { i } = inf * ones ( length ( XXU { i }), 1 ); rob . AA { i } = AA { i }; rob . zz0 { i } = [ vec ( DM ( repmat ( ppNumAll , 1 , N ))); zeros ( 2 * N , 1 )]; end Distributed MPC with ALADIN- \\alpha \\alpha After setting up some options, the discretized OCP can be solved with ALADIN- \\alpha \\alpha . Here we do that within an Model Predictive Control loop, where we use the reuse option of ALADIN- \\alpha \\alpha in order to to construct the derivatives and local solvers only once. Note that the initial position of the robots changes in each iteration, cf. [2] for more information on MPC. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 rob . lam0 = 0 * ones ( size ( AA { 1 }, 1 ), 1 ); rob . p = ppNumAll ; opts . plot = 'false' ; opts . reuse = 'true' ; opts . maxiter = 50 ; opts . term_eps = 1e-8 ; Xopt = ppNumAll ; for i = 1:Nmpc sol_rob { i } = run_ALADINnew ( rob , opts ); Xopti = []; for j = 1:Nrobot Xopti = [ Xopti ; full ( sol_rob { i }. xxOpt { j }( 3 * ( Nrobot + j - 1 ) + ( 1 : 3 )))]; end Xopt = [Xopt, Xopti] ; rob . zz0 = sol_rob { i }. xxOpt ; rob . p = Xopt (:, i + 1 ); % reuse problem formulation fNames = fieldnames ( sol_rob { 1 }. problemForm ); for i = 1:length(fNames) rob .( fNames { i }) = sol_rob { 1 }. problemForm .( fNames { i }); end end To see the advantage of distributed parametric programming in combination with the reuse option, we can have a look at the computation times. In the first iteration, ALADIN- \\alpha \\alpha needs .8 seconds for the problem setup and .7 seconds for iterating. After the second iteration however, the time for problem setup will be 0 and also the iteration time is halfed on my computer to .3 seconds due to the fact that also the previous solution is used as an initial guess for ALADIN- \\alpha \\alpha . This shows how the reuse option of ALADIN- \\alpha \\alpha can be used to make distributed MPC more efficient. The resulting closed-loop trajectories are shown in the following figure. Not that the distance constraint {\\|(z, y)_i^\\top(t)-(z, y)_j^\\top(t)\\|_2^2}\\geq d^2 {\\|(z, y)_i^\\top(t)-(z, y)_j^\\top(t)\\|_2^2}\\geq d^2 is satisfied while the robots exchange their position. Decentralized MPC Not that for decentralized MPC, the bi-level variants with decentralized ADMM and decentralized conjugate gradients can be used which can be activated via the option innerAlg . References [1] Engelmann, A., Jiang, Y., Houska, B., & Faulwasser, T. (2019). Decomposition of non-convex optimization via bi-level distributed ALADIN. arXiv preprint arXiv:1903.11280. [2] Rawlings, J. B., & Mayne, D. Q., Diehl, M. (2017). Model Predictive Control: Theory, Computation and Design. Nob Hill Pub.","title":"Distributed MPC"},{"location":"robotEx/#distributed-mpc-for-mobile-robots","text":"Here we give an example how ALADIN-M can be used for distributed Model Predictive Control. In particular, we show how the distributed parametric programming option and the the problem reuse option of ALADIN-M are useful. The example is similar to the example from [1]. The goal here is that two mobile robots exchange their positions while keeping a certain distance. This task can be formulated as a continuous-time optimal control problem (OCP) \\begin{aligned} &\\min_{x_i(),u_i(), \\forall i \\in \\mathcal{R}} \\int_0^T \\sum_{i\\in \\mathcal{R}} \\|x_i-x^e_i\\|_{Q_i}^2 + \\|u_i\\|_{R_i}^2\\, dt \\\\ \\quad \\text{s.t.} \\quad & \\dot x_i(t) = f_i(x_i(t),u_i(t)), x_i(0)=z_{i0}, && \\forall i \\in \\mathcal{R} \\\\ &(z,y)_i^\\top(T)=(z^e,y^e)_i^\\top, &&\\forall i \\in \\mathcal{R} \\\\ & \\|(z, y)_i^\\top(t)-(z, y)_j^\\top(t)\\|_2^2\\geq d^2, && i \\neq j. \\end{aligned} \\begin{aligned} &\\min_{x_i(),u_i(), \\forall i \\in \\mathcal{R}} \\int_0^T \\sum_{i\\in \\mathcal{R}} \\|x_i-x^e_i\\|_{Q_i}^2 + \\|u_i\\|_{R_i}^2\\, dt \\\\ \\quad \\text{s.t.} \\quad & \\dot x_i(t) = f_i(x_i(t),u_i(t)), x_i(0)=z_{i0}, && \\forall i \\in \\mathcal{R} \\\\ &(z,y)_i^\\top(T)=(z^e,y^e)_i^\\top, &&\\forall i \\in \\mathcal{R} \\\\ & \\|(z, y)_i^\\top(t)-(z, y)_j^\\top(t)\\|_2^2\\geq d^2, && i \\neq j. \\end{aligned} Here, z_i=(x_i\\; y_i\\; \\theta_i)^\\top z_i=(x_i\\; y_i\\; \\theta_i)^\\top is the state of each robot i \\in \\mathcal{R} i \\in \\mathcal{R} , x_i x_i and y_i y_i describe the robots position in the x x - y y -plane, and \\theta_i \\theta_i is the yaw angle with respect to the x x -axis (cf. figure below). Note that the initial condition can here be interpreted as the parameter p_i p_i in the format suitable for ALADIN- \\alpha \\alpha . The robots\u2019 dynamics is given by $$ \\dot x_i= f_i(x_i,u_i) := \\begin{pmatrix} v_i\\cos (\\theta_i) & v_i\\sin(\\theta_i) & \\omega_i \\end{pmatrix}^\\top , \\; %\\begin{pmatrix} %\\cos (\\theta_i) & 0 \\ %\\sin(\\theta_i) & 0 \\ %0 & 1 %\\end{pmatrix} %\\begin{pmatrix} %v_i \\ \\omega_i %\\end{pmatrix}, \\quad i \\in {1,2}. $$","title":"Distributed MPC for Mobile Robots"},{"location":"robotEx/#distributed-problem-setup","text":"Code-wise, we set up the right-hand-side of the ode dynamics from above as 1 2 3 4 % define robot models ode = @( x , u ) [ u ( 1 ) * cos ( x ( 3 )); u ( 1 ) * sin ( x ( 3 )); u ( 2 )]; Next, we prepare ourself for setting ob the robots\u2019 OCPs. We construct our problem using CasADi. Specifically, we create cells XX and UU containing the CasADi symbolic variables for the states z z and the inputs u u over the horizon-length T T . Moreover, we introduce a cell with state copies ZZZ containing the state information of neighboring robots in order to allow each robot to fulfill the distance inequality constraint {\\|(z, y)_i^\\top(t)-(z, y)_j^\\top(t)\\|_2^2}\\geq d^2 {\\|(z, y)_i^\\top(t)-(z, y)_j^\\top(t)\\|_2^2}\\geq d^2 . Note that we will enforce the copied and the original state to coincide later by the consensus constraint \\sum_{i\\in \\mathcal{R}} A_ix_i=b \\sum_{i\\in \\mathcal{R}} A_ix_i=b in the ALADIN- \\alpha \\alpha format . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 %% set up OCP import casadi.* Nrobot = 2 ; T = 1 ; % Time horizon dT = 0.1 ; % sampling time d = 2 ; % minimal distance between robots N = T/dT ; Nmpc = 40 ; for i=1:Nrobot % inputs/states XX { i } = SX . sym ([ 'x' num2str ( i )], [ 3 N ]); % state copies for k=setdiff(1:Nrobot,i) ZZZ { i }{ k } = SX . sym ([ 'z' num2str ( i ) num2str ( k )], [ 3 N ]); end UU{i} = SX.sym(['u' num2str(i)], [2 N]) ; XX0 { i } = SX . sym ([ 'xx0' num2str ( i )], [ 3 1 ]); end Now we are ready to set-up a discretized version of the above OCP. Note that we construct this OCP in a loop setting up the OCPs for all robots individually. Here we use a Heun-discretization scheme. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 % starting points/destinations % from to ppNum { 1 } = [ 0 10 ; 0 10 0 0 ] ; ppNum { 2 } = [ 10 0 ; 10 0 ; 0 0 ] ; % for each robot ... for i = 1 : Nrobot JJ { i } = 0 ; gg { i } = []; hh { i } = []; llbu { i } = []; uubu { i } = []; % over horizon ... for j=1:N - 1 % ode/stage cost with Heun discretization gg { i } = [ gg { i }; XX { i }(:, j + 1 ) - XX { i }(:, j ) - dT * 0.5 * ( ode ( XX { i }(:, j ), UU { i }(:, j )) + ode ( XX { i }(:, j + 1 ), UU { i }(:, j + 1 )))]; JJ { i } = JJ { i } + ( XX { i }(:, j ) - ppNum { i }(:, 2 )) '* diag ([ 1 1 0 ]) * ( XX { i }(:, j ) - ppNum { i }(:, 2 )) ... + UU { i }(:, j ) '* UU { i }(:, j ); % distance constraint hh { i } = [ hh { i }; - ( XX { i }( 1 : 2 , j ) - ZZZ { i }{ 3 - i }( 1 : 2 , j )) '* ( XX { i }( 1 : 2 , j ) - ZZZ { i }{ 3 - i }( 1 : 2 , j )) + d ^ 2 ]; end % initial condition gg { i } = [ gg { i }; XX { i }(:, 1 ) - XX0 { i }]; end for i=1:Nrobot ZZZ { i }{ i } = XX { i }; ZZZi = vertcat(vertcat(ZZZ{i}{:})) ; UUi = UU{i} ; XXU { i } = [ ZZZi (:); UUi (:)]; end Next, we construct the consensus matrices \\{A_i\\}_{i \\in \\mathcal{R}} \\{A_i\\}_{i \\in \\mathcal{R}} . As mentioned before, we construct them such that the original trajectories coincide with the copied trajectories. 1 2 3 4 5 6 7 % set up consensus constraints Abase = [ eye(Nrobot*N*3) zeros(Nrobot*N*3,2*N)] ; zerBase = zeros ( size ( Abase )); for i=1:Nrobot-1 AA { i } = [ repmat ( zerBase , i - 1 , 1 ); Abase ; repmat ( zerBase , Nrobot - i - 1 , 1 )]; end AA{Nrobot} = - repmat(Abase,Nrobot-1,1) ; In a last step, we convert the CasADi symbolic expressions to evaluable and set up the initial guesses z_i^0 z_i^0 and \\lambda^0 \\lambda^0 . Note that the local equality constraints collected in rob.locFuns.gg are parametrized with the initial condition X0 for the ode of the robots here. With that, we will be able to efficiently reuse the problem formulation in an MPC loop as we shall see next. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 % convert expressions to MATLAB functions X0 = vertcat(XX0{:}) ; ppNumAll = [ ppNum { 1 }(:, 1 ); ppNum { 2 }(:, 1 )]; for i=1:Nrobot rob . locFuns . ffi { i } = Function ([ 'f' num2str ( i )],{ XXU { i }},{ JJ { i }}); rob . locFuns . ggi { i } = Function ([ 'g' num2str ( i )],{[ XXU { i }; X0 ]},{ gg { i }}); rob . locFuns . hhi { i } = Function ([ 'h' num2str ( i )],{ XXU { i }},{ hh { i }}); % set up ALADIN parameters rob . llbx { i } = - inf * ones ( length ( XXU { i }), 1 ); rob . uubx { i } = inf * ones ( length ( XXU { i }), 1 ); rob . AA { i } = AA { i }; rob . zz0 { i } = [ vec ( DM ( repmat ( ppNumAll , 1 , N ))); zeros ( 2 * N , 1 )]; end","title":"Distributed Problem Setup"},{"location":"robotEx/#distributed-mpc-with-aladin-alphaalpha","text":"After setting up some options, the discretized OCP can be solved with ALADIN- \\alpha \\alpha . Here we do that within an Model Predictive Control loop, where we use the reuse option of ALADIN- \\alpha \\alpha in order to to construct the derivatives and local solvers only once. Note that the initial position of the robots changes in each iteration, cf. [2] for more information on MPC. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 rob . lam0 = 0 * ones ( size ( AA { 1 }, 1 ), 1 ); rob . p = ppNumAll ; opts . plot = 'false' ; opts . reuse = 'true' ; opts . maxiter = 50 ; opts . term_eps = 1e-8 ; Xopt = ppNumAll ; for i = 1:Nmpc sol_rob { i } = run_ALADINnew ( rob , opts ); Xopti = []; for j = 1:Nrobot Xopti = [ Xopti ; full ( sol_rob { i }. xxOpt { j }( 3 * ( Nrobot + j - 1 ) + ( 1 : 3 )))]; end Xopt = [Xopt, Xopti] ; rob . zz0 = sol_rob { i }. xxOpt ; rob . p = Xopt (:, i + 1 ); % reuse problem formulation fNames = fieldnames ( sol_rob { 1 }. problemForm ); for i = 1:length(fNames) rob .( fNames { i }) = sol_rob { 1 }. problemForm .( fNames { i }); end end To see the advantage of distributed parametric programming in combination with the reuse option, we can have a look at the computation times. In the first iteration, ALADIN- \\alpha \\alpha needs .8 seconds for the problem setup and .7 seconds for iterating. After the second iteration however, the time for problem setup will be 0 and also the iteration time is halfed on my computer to .3 seconds due to the fact that also the previous solution is used as an initial guess for ALADIN- \\alpha \\alpha . This shows how the reuse option of ALADIN- \\alpha \\alpha can be used to make distributed MPC more efficient. The resulting closed-loop trajectories are shown in the following figure. Not that the distance constraint {\\|(z, y)_i^\\top(t)-(z, y)_j^\\top(t)\\|_2^2}\\geq d^2 {\\|(z, y)_i^\\top(t)-(z, y)_j^\\top(t)\\|_2^2}\\geq d^2 is satisfied while the robots exchange their position.","title":"Distributed MPC with ALADIN-\\alpha\\alpha"},{"location":"robotEx/#decentralized-mpc","text":"Not that for decentralized MPC, the bi-level variants with decentralized ADMM and decentralized conjugate gradients can be used which can be activated via the option innerAlg .","title":"Decentralized MPC"},{"location":"robotEx/#references","text":"[1] Engelmann, A., Jiang, Y., Houska, B., & Faulwasser, T. (2019). Decomposition of non-convex optimization via bi-level distributed ALADIN. arXiv preprint arXiv:1903.11280. [2] Rawlings, J. B., & Mayne, D. Q., Diehl, M. (2017). Model Predictive Control: Theory, Computation and Design. Nob Hill Pub.","title":"References"}]}